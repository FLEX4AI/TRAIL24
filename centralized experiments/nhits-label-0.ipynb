{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9083908,"sourceType":"datasetVersion","datasetId":5480827},{"sourceId":9085069,"sourceType":"datasetVersion","datasetId":5481656}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook trains NHITS model on the first cluster buildings. labeled-dataset contains the data of all the buildings in the first cluster","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Function to optimize data types\ndef optimize_data_types(df):\n    # Optimize numeric columns\n    for col in df.select_dtypes(include=['int']).columns:\n        df[col] = df[col].astype('int32')\n    \n    for col in df.select_dtypes(include=['float']).columns:\n        df[col] = df[col].astype('float32')\n\n    # Optimize object columns\n    for col in df.select_dtypes(include=['object']).columns:\n        num_unique_values = len(df[col].unique())\n        num_total_values = len(df[col])\n        if num_unique_values / num_total_values < 0.5:\n            df[col] = df[col].astype('category')\n    \n    return df\n\n# Load the data and remove the first and last columns\ndf = pd.read_csv('/kaggle/input/labeled-dataset/df_labeled0.csv')\ndf = df.iloc[:, 1:-1]\n\n# Optimize data types\ndf = optimize_data_types(df)\n\n# Verify DataFrame shape\nprint(\"DataFrame shape:\", df.shape)\nprint(\"DataFrame head:\", df.head())\n\n# Define the window size for lagged inputs\nwindow_size = 40  # Replace N with your desired window size\n\n# Function to process each chunk\ndef process_chunk(chunk, window_size):\n    data = []\n    for i, row in chunk.iterrows():\n        values = row.values\n        for t in range(window_size, len(values)):\n            lagged_inputs = values[t-window_size:t]\n            target_value = values[t]\n\n            timestamp = pd.Timestamp('2009-07-14 00:00:00') + pd.Timedelta(hours=t)\n            year = timestamp.year\n            month = timestamp.month\n            day = timestamp.day\n            hour = timestamp.hour\n\n            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n    return data\n\n# Create an empty DataFrame to store results\ncolumns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\ndf_transformed = pd.DataFrame(columns=columns)\n\n# Process the data in chunks\nchunk_size = 50  # Adjust based on available memory\nfor start_row in range(0, df.shape[0], chunk_size):\n    chunk = df.iloc[start_row:start_row + chunk_size]\n    data = process_chunk(chunk, window_size)\n    df_chunk_transformed = pd.DataFrame(data, columns=columns)\n    df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n\n\nordinal_encoder = OrdinalEncoder()\ndf_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n\n\nprint(\"Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T21:37:05.855614Z","iopub.execute_input":"2024-08-04T21:37:05.856590Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"DataFrame shape: (1523, 12865)\nDataFrame head:    2009-07-14 00:00:00  2009-07-14 01:00:00  2009-07-14 02:00:00  \\\n0                0.692                0.761                0.725   \n1                0.177                0.324                0.317   \n2                0.048                0.183                0.101   \n3                0.204                0.603                0.334   \n4                0.275                0.341                0.224   \n\n   2009-07-14 03:00:00  2009-07-14 04:00:00  2009-07-14 05:00:00  \\\n0                0.546                0.729                0.755   \n1                0.311                0.305                0.304   \n2                0.175                1.062                0.322   \n3                0.348                0.758                0.479   \n4                0.360                0.378                0.335   \n\n   2009-07-14 06:00:00  2009-07-14 07:00:00  2009-07-14 08:00:00  \\\n0                0.773                0.743                0.911   \n1                0.303                0.300                1.213   \n2                0.151                0.241                0.157   \n3                0.438                0.451                0.863   \n4                0.447                0.442                0.781   \n\n   2009-07-14 09:00:00  ...  2010-12-31 15:00:00  2010-12-31 16:00:00  \\\n0                0.579  ...                1.901                2.744   \n1                0.730  ...                1.540                4.620   \n2                0.117  ...                6.229                5.182   \n3                0.262  ...                0.616                1.384   \n4                0.583  ...                0.808                1.340   \n\n   2010-12-31 17:00:00  2010-12-31 18:00:00  2010-12-31 19:00:00  \\\n0                4.013                3.007                2.166   \n1                5.607                6.813                4.491   \n2                6.428                7.117                6.624   \n3                2.514                2.756                2.379   \n4                1.277                1.717                1.127   \n\n   2010-12-31 20:00:00  2010-12-31 21:00:00  2010-12-31 22:00:00  \\\n0                1.617                1.636                1.700   \n1                1.505                1.457                1.089   \n2                3.757                2.395                2.286   \n3                3.265                3.509                3.349   \n4                1.143                0.984                0.885   \n\n   2010-12-31 23:00:00  2011-01-01 00:00:00  \n0                1.674                0.775  \n1                1.124                0.611  \n2                2.213                1.142  \n3                3.359                1.561  \n4                0.909                0.434  \n\n[5 rows x 12865 columns]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2528631983.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"######CORRECT IT\nimport pandas as pd\nimport gc\n# Convert the date columns to datetime\ndf_transformed['year1'] = df_transformed['year']\ndf_transformed['month1'] = df_transformed['month']\ndf_transformed['day1'] = df_transformed['day']\ndf_transformed['hour1'] = df_transformed['hour']\ndf_transformed[\"target1\"] = df_transformed[\"target\"]\n\n# Define the date ranges for training and testing\ntrain_start_date = '2009-07-14'\ntrain_end_date = '2010-12-15'\ntest_start_date = '2010-12-15'\ntest_end_date = '2011-01-01'\n\n# Convert the date columns to datetime\ndf_transformed['year'] = df_transformed['year'].astype(int)\ndf_transformed['month'] = df_transformed['month'].astype(int)+1\ndf_transformed['day'] = df_transformed['day'].astype(int)+1\ndf_transformed['hour'] = df_transformed['hour'].astype(int)+1\n\n# Create a datetime column\ndf_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\ndf_sample = df_transformed.loc[:12825]\n\n# Set the datetime column as the index\ndf_transformed.set_index('date_time', inplace=True)\ndf_sample.set_index('date_time', inplace=True)\n\n# Drop the irrelevant columns\ndf_transformed.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\ndf_sample.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n\ndf_transformed = df_transformed.sort_index()\ndf_sample = df_sample.sort_index()\n\n# Split the data into training and testing sets\ntrain = df_transformed.loc[train_start_date:train_end_date]\n\ntest = df_transformed.loc[test_start_date:test_end_date]\n\n\n# Create sample data for the first building\nsample = df_sample.loc[train_start_date:train_end_date]\n\n\n\ndel df_transformed\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:29:17.652082Z","iopub.execute_input":"2024-08-04T01:29:17.652525Z","iopub.status.idle":"2024-08-04T01:29:33.229563Z","shell.execute_reply.started":"2024-08-04T01:29:17.652490Z","shell.execute_reply":"2024-08-04T01:29:33.228393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:29:47.618845Z","iopub.execute_input":"2024-08-04T01:29:47.619244Z","iopub.status.idle":"2024-08-04T01:29:47.977880Z","shell.execute_reply.started":"2024-08-04T01:29:47.619199Z","shell.execute_reply":"2024-08-04T01:29:47.976787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nfrom memory_profiler import memory_usage\nimport resource\nimport pickle\nimport gc\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Define a function to create the model for memory profiling\ndef create_model():\n    model = CombinedModel(nhits_params, embedding_dim, final_hidden)\n    return model\n\ndef get_memory_usage():\n    # Return current memory usage in MB\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n\nclass NHiTSBlock(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers):\n        super(NHiTSBlock, self).__init__()\n        self.hidden = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n                                    [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n        self.theta_b = nn.Linear(num_hidden, input_size)\n        self.theta_f = nn.Linear(num_hidden, output_size)\n\n    def forward(self, x):\n        for layer in self.hidden:\n            x = torch.relu(layer(x))\n        backcast = self.theta_b(x)\n        forecast = self.theta_f(x)\n        return backcast, forecast\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size, output_size, num_blocks, num_hidden, num_layers):\n        super(NHiTS, self).__init__()\n        self.blocks = nn.ModuleList([NHiTSBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks)])\n\n    def forward(self, x):\n        forecast = torch.zeros((x.size(0), self.blocks[0].theta_f.out_features), device=x.device)\n        for block in self.blocks:\n            backcast, block_forecast = block(x)\n            x = x - backcast\n            forecast = forecast + block_forecast\n        return forecast\n\nclass EmbeddingNetwork(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super(EmbeddingNetwork, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, x):\n        return self.embedding(x)\n\nclass YearNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super(YearNetwork, self).__init__()\n        self.hidden = nn.Linear(input_dim, 25)\n        self.output = nn.Linear(25, num_hidden)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden(x))\n        return self.output(x)\n\nclass CombinedModel(nn.Module):\n    def __init__(self, nhits_params, embedding_dim, final_hidden):\n        super(CombinedModel, self).__init__()\n        self.nhits = NHiTS(**nhits_params)\n        self.month_net = EmbeddingNetwork(12, embedding_dim)  # Months from 1 to 12\n        self.day_net = EmbeddingNetwork(31, embedding_dim)    # Days from 1 to 31\n        self.hour_net = EmbeddingNetwork(24, embedding_dim)   # Hours from 0 to 23\n        self.year_net = YearNetwork(1, 10)\n        self.final_layer = nn.Sequential(\n            nn.Linear(embedding_dim * 3 + nhits_params['output_size'] + 10, final_hidden),\n            nn.ReLU(),\n            nn.Linear(final_hidden, 1)\n        )\n\n    def forward(self, x, month, day, hour, year):\n        ts_output = self.nhits(x)\n        month_output = self.month_net(month.long()).squeeze(1)\n        day_output = self.day_net(day.long()).squeeze(1)\n        hour_output = self.hour_net(hour.long()).squeeze(1)\n        year_output = self.year_net(year.float())\n        combined_output = torch.cat((ts_output, month_output, day_output, hour_output, year_output), dim=1)\n        final_output = self.final_layer(combined_output)\n        return final_output\n\n\n# Example usage:\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 512\n\nnhits_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\nstart_mem = get_memory_usage()\nmodel = create_model()\nend_mem = get_memory_usage()\nprint(f\"Memory used for model creation: {end_mem - start_mem} MB\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].astype(\"float32\")\n        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n        year = torch.FloatTensor([row[-5]]).to(torch.int)\n        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is already 0-indexed by OrdinalEncoder\n        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is already 0-indexed by OrdinalEncoder\n        hour = torch.FloatTensor([row[-2]]).to(torch.int)   # Hour is already 0-indexed by OrdinalEncoder\n        label = torch.FloatTensor([row[-1]])  # The last column\n        return features, year, month, day, hour, label\n\n    def __len__(self):\n        return len(self.dataframe)\n\ndef create_data_loaders(data, chunk_size, batch_size):\n    chunks = np.array_split(data, len(data) // chunk_size)\n    data_loaders = []\n    for chunk in chunks:\n        dataset = CustomDataset(chunk)\n        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        data_loaders.append(data_loader)\n    return data_loaders\n\ndevice = \"cuda\"\nlr = 0.0005\nn_epochs = 10\nwindow_size = 40\nchunk_size = 10000\nbatch_size = 1024\n\n# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\ntrain_loaders = create_data_loaders(train, chunk_size, batch_size)\ntest_loaders = create_data_loaders(test, chunk_size, batch_size)\nsample_loaders = create_data_loaders(sample, chunk_size, batch_size)\nprint(\"data loaders are ready\")\n\ndel train, test, sample\ngc.collect()\n\ndef train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cpu\")):\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        counter = 0\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in data_loader:\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                loss = criterion(outputs, labels)\n                epoch_loss += loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        scheduler.step(epoch_loss / len(data_loaders))\n        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n        collected = gc.collect()\n    return net\n\ndef test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    net.eval()\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in data_loader:\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                outputs = outputs.squeeze()\n                labels = labels.squeeze()\n\n                mse(outputs, labels)\n                smape(outputs, labels)\n                list_targets.append(labels.detach())\n                list_outputs.append(outputs.detach())\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n    if return_data:\n        return torch.cat(list_outputs), torch.cat(list_targets), test_mse\n\ncriterion = nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nprint(\"Model Started\")\n\nstart_mem = get_memory_usage()\ntime1 = time.time()\nnet = train_function(model, criterion, optimizer, train_loaders, n_epochs=n_epochs, device=device)\ntime2 = time.time()\nprint(\"training time is \", time2 - time1)\nend_mem = get_memory_usage()\nprint(f\"Memory used for model training: {end_mem - start_mem} MB\")\n\nnet.to(\"cuda\")\ntime3 = time.time()\ntest_function(net, test_loaders, None, None, torch.device(\"cuda\"))\ntime4 = time.time()\nprint(\"inference time is \", time4 - time3)\n\nsample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(\"cuda\"), True)\nplt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NHiTS Predictions\", markersize=3)\nplt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state first building\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T01:29:55.470174Z","iopub.execute_input":"2024-08-04T01:29:55.470643Z"},"trusted":true},"execution_count":null,"outputs":[]}]}