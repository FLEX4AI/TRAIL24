[
  {
    "objectID": "experiment.interpret.html",
    "href": "experiment.interpret.html",
    "title": "Interpretation",
    "section": "",
    "text": "# Apply t-SNE\ntsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)\ntsne_results = tsne.fit_transform(data_scaled)\n\n# Create a DataFrame with t-SNE results and cluster labels\ntsne_df = pd.DataFrame({\n    't-SNE1': tsne_results[:, 0],\n    't-SNE2': tsne_results[:, 1],\n    'Cluster': labels\n})\n\n# Plotting\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Cluster'], cmap='viridis', s=50)\nplt.title('t-SNE Plot of Building Energy Consumption Clusters')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.legend(handles=scatter.legend_elements()[0], labels=set(labels))\nplt.colorbar(scatter, label='Cluster Label')\nplt.show()\n\n\ndf_labels = pd.DataFrame(labels, columns=['labels'])\ndf_interpret = pd.concat([df_aggregations, df_labels], axis=1)\n\n\n#how many buildings in each cluster\ndf_interpret[\"labels\"].value_counts()\n\n\n#interpreting the results of first cluster\ndf_interpret[df_interpret[\"labels\"] == 0].describe()\n\n\n#interpreting the results of second cluster\ndf_interpret[df_interpret[\"labels\"] == 1].describe()\n\n\n#interpreting the results of third cluster\ndf_interpret[df_interpret[\"labels\"] == 2].describe()",
    "crumbs": [
      "Experiments",
      "Interpretation"
    ]
  },
  {
    "objectID": "data.cluster.html",
    "href": "data.cluster.html",
    "title": "Cluster",
    "section": "",
    "text": "data.cluster\n\nFill in a module description here\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\nsource\n\nresample_building_data\n\n resample_building_data (group)\n\n\nsource\n\n\ncalculate_wcss\n\n calculate_wcss (data:list[float], max_k:int)\n\ncompute the WCSS metric\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nlist\nthe input dataframe\n\n\nmax_k\nint\nthe number of clusters\n\n\nReturns\nfloat\n\n\n\n\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/func_comments\n\nsource\n\n\nplot_elbow_method\n\n plot_elbow_method (wcss:list[float], max_k:int)\n\nplot the graph of the wcss metric as a function of the number of clusters\n\n\n\n\nType\nDetails\n\n\n\n\nwcss\nlist\nthe wcss metric to plot\n\n\nmax_k\nint\nthe number of clusters\n\n\n\n\nsource\n\n\ncompute_statistics\n\n compute_statistics (data:list[float])\n\ncompute multiple statistics like mean, std, skewness, kurtosis, etc\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nlist\nthe input dataframe\n\n\nReturns\ntuple\nmultiple outputs\n\n\n\n\nsource\n\n\nweekly_monthly_statistics\n\n weekly_monthly_statistics (data:pandas.core.series.Series)\n\nCompute some statistics (mean, std, skewness, kurtosis, energy, periodicity)\n\nsource\n\n\nrevert_umap_projection_2D\n\n revert_umap_projection_2D (umap_value_1:float, umap_value_2:float,\n                            umap_df_2d:pandas.core.frame.DataFrame)\n\nFind the data that corresponds to the UMAP projection in 2D\n\n\n\n\nType\nDetails\n\n\n\n\numap_value_1\nfloat\nfirst umap coeff\n\n\numap_value_2\nfloat\nsecond umap coeff\n\n\numap_df_2d\nDataFrame\n\n\n\nReturns\nint\n\n\n\n\n\nsource\n\n\nrevert_umap_projection_3D\n\n revert_umap_projection_3D (umap_value_1, umap_value_2, umap_value_3,\n                            umap_df_3d:pandas.core.frame.DataFrame)\n\nFind the data that corresponds to the UMAP projection in 3D\n\n\n\n\nType\nDetails\n\n\n\n\numap_value_1\n\nfirst umap coeff\n\n\numap_value_2\n\nsecond umap coeff\n\n\numap_value_3\n\nthird umap coeff\n\n\numap_df_3d\nDataFrame\n\n\n\nReturns\nint",
    "crumbs": [
      "Data",
      "Cluster"
    ]
  },
  {
    "objectID": "compression.pruning.html",
    "href": "compression.pruning.html",
    "title": "Pruning",
    "section": "",
    "text": "compression.pruning\n\nFill in a module description here\n\n\nsource\n\nget_ignored_layers\n\n get_ignored_layers (model)\n\n\nsource\n\n\nadjust_layer_features\n\n adjust_layer_features (layer, pruning_ratio)\n\n\nsource\n\n\nprune_model\n\n prune_model (model, pruning_ratio, dummy_input)\n\n\nfrom TRAIL24.models.nn import *\n\n\n# Example usage:\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 512\n\nnbeats_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\nmodel_cfg = {\n    'model_type': 'nbeats', \n    'model_params': nbeats_params, \n    'embedding_dim': 10, \n    'final_hidden': 256\n}\n\n\nnet = create_model(**model_cfg)\n\n\nnum_parameters = get_num_parameters(net)\ndisk_size = get_model_size(net)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 90.39 MB (disk), 22576523 parameters\n\n\n\nbatch_size = 5\nnum_features = 40\n\nfeatures = torch.randn(batch_size, num_features)\n\n\nprune_model(net, 0.3, features)\n\ntorch.Size([5, 40])\n\n\n\nnum_parameters = get_num_parameters(net)\ndisk_size = get_model_size(net)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 44.71 MB (disk), 11157731 parameters"
  },
  {
    "objectID": "models.nn.html",
    "href": "models.nn.html",
    "title": "Models",
    "section": "",
    "text": "models.nn\n\nFill in a module description here\n\n\nsource\n\nYearNetwork\n\n YearNetwork (input_dim, num_hidden)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nEmbeddingNetwork\n\n EmbeddingNetwork (num_embeddings, embedding_dim)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nNHiTS\n\n NHiTS (input_size, output_size, num_blocks, num_hidden, num_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nNHiTSBlock\n\n NHiTSBlock (input_size, output_size, num_hidden, num_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nNBEATS\n\n NBEATS (input_size, output_size, num_hidden, num_layers, num_blocks)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nSeasonalityBlock\n\n SeasonalityBlock (input_size, output_size, num_hidden, num_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nTrendBlock\n\n TrendBlock (input_size, output_size, num_hidden, num_layers)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\ncreate_model\n\n create_model (model_type, model_params, embedding_dim, final_hidden)\n\n\nsource\n\n\nCombinedModel\n\n CombinedModel (model_type, model_params, embedding_dim, final_hidden)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "experiment.train.html",
    "href": "experiment.train.html",
    "title": "Training",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nimport resource\nimport pickle\nimport gc\nimport warnings\nfrom tqdm.notebook import tqdm\nimport mlflow\nfrom mlflow.models import infer_signature\n\n\nmlflow.set_tracking_uri(uri=\"https://mlflow.tsw24.cetic.be\")\n\n\nfrom TRAIL24.TRAIL24.data.preprocess import *\nfrom TRAIL24.TRAIL24.models.nn import *\n\n\nwith open('../data/df_cluster_0.pkl', 'rb') as f:\n        df = pickle.load(f)\n\nnum_rows = len(df)\n\n# Calculate the halfway point\nhalfway_point = num_rows // 6\n\n# Select the first half of the rows\ndf = df.iloc[:halfway_point]\n\n# Preprocess the DataFrame\ndf_transformed = preprocess_dataframe(df, window_size=40, chunk_size=100)\n\n\n# Define date ranges\ntrain_start_date = '2009-07-14'\ntrain_end_date = '2010-12-15'\ntest_start_date = '2010-12-15'\ntest_end_date = '2011-01-01'\n\n# Finalize the DataFrame (split into train and test)\ntrain, test = finalize_dataframe(df_transformed, train_start_date, train_end_date, test_start_date, test_end_date)\n\n# Scale the first 40 columns\ncolumns_to_scale = [f'lag_{i}' for i in range(1, 41)]\ntrain, test = scale_features(train, test, columns_to_scale)\n\n# Perform memory cleanup\ndel df_transformed, df\ngc.collect()\n\nprint(\"Data preprocessing complete.\")\n\n\n\n\n\n\n\n\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].astype(\"float32\")\n        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n        year = torch.FloatTensor([row[-5]]).to(torch.int)\n        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is expected to be 0-11\n        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is expected to be 0-30\n        hour = torch.FloatTensor([row[-2]]).to(torch.int)      # Hour is already 0-indexed (0-23)\n        label = torch.FloatTensor([row[-1]])  # The last column\n        return features, year, month, day, hour, label\n\n    def __len__(self):\n        return len(self.dataframe)\n\ndef create_data_loaders(data, chunk_size, batch_size):\n    chunks = np.array_split(data, len(data) // chunk_size)\n    data_loaders = []\n    for chunk in chunks:\n        dataset = CustomDataset(chunk)\n        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        data_loaders.append(data_loader)\n    return data_loaders\n\n\ndef train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cuda\")):\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n    for epoch in tqdm(range(n_epochs)):\n        epoch_loss = 0\n        counter = 0\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in tqdm(data_loader):\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                loss = criterion(outputs, labels)\n                epoch_loss += loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        print(f\"epoch {epoch}\")\n        scheduler.step(epoch_loss / len(data_loaders))\n        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n        collected = gc.collect()\n    return net\n\ndef test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    net.eval()\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():  # to not reserve a memory space for gradients\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in tqdm(data_loader):\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                outputs = outputs.squeeze()\n                labels = labels.squeeze()\n\n                mse(outputs, labels)\n                smape(outputs, labels)\n                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n    if return_data:\n        return torch.cat(list_outputs), torch.cat(list_targets), test_mse, test_smape\n\n\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 512\n\n\nnbeats_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\nmodel_cfg = {\n    'model_type': 'nbeats', \n    'model_params': nbeats_params, \n    'embedding_dim': 10, \n    'final_hidden': 256\n}\n\nnet = create_model(**model_cfg)\n\ndevice = \"cuda\"\nnet.to(device)\n\nlr = 0.0005\nn_epochs = 10\nwindow_size = 40\nchunk_size = 122880\nbatch_size = 1024*12*5\n\n\n# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\ntrain_loaders = create_data_loaders(train, chunk_size, batch_size)\ntest_loaders = create_data_loaders(test, chunk_size, batch_size)\n#sample_loaders = create_data_loaders(sample, 100, 1)\nprint(\"data loaders are ready\")\n\n#del train, test, sample\ndel train, test\ngc.collect()\n\n/home/HubensN/miniconda3/envs/dev/lib/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 4\n      2 train_loaders = create_data_loaders(train, chunk_size, batch_size)\n      3 test_loaders = create_data_loaders(test, chunk_size, batch_size)\n----&gt; 4 sample_loaders = create_data_loaders(sample, 100, 1)\n      5 print(\"data loaders are ready\")\n      7 del train, test, sample\n\nNameError: name 'sample' is not defined\n\n\n\n\ncriterion = nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\nnet.to(device)\n\nprint(\"Model Started\")\n\nstart_mem = get_memory_usage()\ntime1 = time.time()\nnet = train_function(net, criterion, optimizer, train_loaders, n_epochs=1, device=torch.device(device))\ntime2 = time.time()\nprint(\"training time is \", time2 - time1)\nend_mem = get_memory_usage()\nprint(f\"Memory used for model training: {end_mem - start_mem} MB\")\n\nnet.to(device)\ntime3 = time.time()\n*_, test_mse, test_smape = test_function(net, test_loaders, None, None, torch.device(device), return_data=True)\ntime4 = time.time()\nprint(\"inference time is \", time4 - time3)\n\n\nsample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(device), True)\nplt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NBEATS Predictions\", markersize=3)\nplt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state 1st building\")\nplt.legend()\nplt.show()\n\nModel Started\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyboardInterrupt\n\n\n\n\n# Create a new MLflow Experiment\nmlflow.set_experiment(\"MLflow Quickstart\")\n\n# Start an MLflow run\nwith mlflow.start_run():\n\n    mlflow.set_tag(\"Training Info\", \"TEST\")\n\n    \n    criterion = nn.MSELoss().to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    \n    net.to(device)\n    \n    print(\"Model Started\")\n    \n    start_mem = get_memory_usage()\n    time1 = time.time()\n    net = train_function(net, criterion, optimizer, train_loaders, n_epochs=1, device=torch.device(device))\n    time2 = time.time()\n    print(\"training time is \", time2 - time1)\n    end_mem = get_memory_usage()\n    print(f\"Memory used for model training: {end_mem - start_mem} MB\")\n    \n    net.to(device)\n    time3 = time.time()\n    *_, test_mse, test_smape = test_function(net, test_loaders, None, None, torch.device(device), return_data=True)\n    time4 = time.time()\n    print(\"inference time is \", time4 - time3)\n\n    mlflow.log_metric(\"test_mse\", test_mse)\n    mlflow.log_metric(\"test_smape\", test_smape)\n    \n    sample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(device), True)\n    plt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NBEATS Predictions\", markersize=3)\n    plt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\n    plt.ylabel(\"Energy Consumption (MW)\")\n    plt.title(f\"Energy Consumption for Electricity state 1st building\")\n    plt.legend()\n    plt.show()\n\n2024/09/04 19:44:02 INFO mlflow.tracking.fluent: Experiment with name 'MLflow Quickstart' does not exist. Creating a new experiment.\n\n\nModel Started\n\n\n\n\n\n\n\n\n\nKeyboardInterrupt\n\n\n\n\nnet = create_model().to(device)\n\n\ntime3 = time.time()\ntest_function(net, test_loaders, None, None, torch.device(device))\ntime4 = time.time()\nprint(\"inference time is \", time4 - time3)\n\nsample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(device), True)\nplt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NBEATS Predictions\", markersize=3)\nplt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state 1st building\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest MSE: 5.135466575622559 , SMAPE 1.9296079874038696\ninference time is  93.09827733039856\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest MSE: 3.5050137042999268 , SMAPE 1.9443151950836182\n\n\n\n\n\n\n\n\n\n\ntorch.save(net, \"saved_model_without_year.pth\")\n\n\nnettt = torch.load(\"saved_model_without_year.pth\")\n\n\nnettt\n\nCombinedModel(\n  (nbeats): NBEATS(\n    (trend_blocks): ModuleList(\n      (0-5): 6 x TrendBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n    (seasonality_blocks): ModuleList(\n      (0-5): 6 x SeasonalityBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n  )\n  (month_net): EmbeddingNetwork(\n    (embedding): Embedding(12, 10)\n  )\n  (day_net): EmbeddingNetwork(\n    (embedding): Embedding(31, 10)\n  )\n  (hour_net): EmbeddingNetwork(\n    (embedding): Embedding(24, 10)\n  )\n  (year_net): YearNetwork(\n    (hidden): Linear(in_features=1, out_features=25, bias=True)\n    (output): Linear(in_features=25, out_features=10, bias=True)\n  )\n  (final_layer): Sequential(\n    (0): Linear(in_features=31, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "experiment.cluster.html",
    "href": "experiment.cluster.html",
    "title": "Clustering",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport umap\nfrom sklearn.manifold import TSNE\n\n2024-09-04 12:50:53.842432: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-09-04 12:50:53.992999: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-04 12:50:53.993106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-04 12:50:54.020472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-09-04 12:50:54.084674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-09-04 12:50:54.940055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\nfrom TRAIL24.data.preprocess import *\nfrom TRAIL24.data.cluster import *\n\npath_to_folder = '/home/vincent/Documents/Multitel/Projects/TRAIL_workshop_Portugal/project/'\n\n\nRead the data and form the dataset\n\nfile_path3 = path_to_folder + 'TRAIL24/dataset_electricity/residential_all.pkl'\ndf3 = pd.read_pickle(file_path3)\ndf3[\"ID\"] = df3[\"ID\"].astype(\"category\")\ndf3[\"time_code\"] = df3[\"time_code\"].astype(\"uint16\")\n\ndf3 = df3.set_index([\"date_time\",\"ID\"])\ndf3 = df3.groupby('ID', group_keys=False, observed=True).apply(resample_building_data)\ndf3=df3.reset_index(level=['ID',\"date_time\"])\n\n# Generate the range of date_time values\nstart_time = pd.Timestamp('2009-07-14 00:00:00')\nend_time = pd.Timestamp('2011-01-01 00:00:00')\ndate_range = pd.date_range(start=start_time, end=end_time, freq='h')\n\n# Pivot the dataset\ndf_pivoted = df3.pivot(index='ID', columns='date_time', values='consumption').reset_index()\n\n# Ensure columns are sorted by date_time\n# df_pivoted = df_pivoted.sort_index(axis=1)\n\n# Optional: Rename the columns to make them more readable\ndf_pivoted.columns.name = None\ndf_pivoted.columns = ['ID'] + [date.strftime('%Y-%m-%d %H:%M:%S') for date in date_range]\n\n\n\nCompute summary statistics\n\naggregation_results = []\n\nfor i, row in tqdm(df_pivoted.iterrows()):\n    building_data = row[1:].astype(float)  # Skip the first column (ID)\n    building_series = pd.Series(building_data.values, index=pd.date_range(start='2009-07-14 00:00:00', periods=len(building_data), freq='h'))\n    \n    # Perform the aggregations\n    #stats = weekly_monthly_statistics(building_series)\n    \n    stats = np.concatenate([compute_statistics(building_series)])\n    \n    # Combine all the aggregation results for the current building\n    aggregation_results.append(stats)\n    \n    \n# Convert the aggregation results to a DataFrame\ndf_aggregations = pd.DataFrame(aggregation_results)#, columns=aggregation_columns)\n\n\n\n\n\n\nClustering\n\nfrom sklearn.cluster import SpectralClustering\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(df_aggregations.fillna(0))\n\nindexes = [0,1,2,3,4,5,6,7,8,9,10]\ndata_scaled_bis = data_scaled[:,indexes]\n\nspectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', n_neighbors=10, random_state=42)\nlabels = spectral.fit_predict(data_scaled_bis)\n\n\n\nPlot clustering results using UMAP and t-SNE\n\n# Apply t-SNE\ntsne = TSNE(n_components=3, perplexity=30, learning_rate=500 , n_iter=1000, random_state=42)\ntsne_results = tsne.fit_transform(data_scaled_bis)\n\n# Create a DataFrame with t-SNE results and cluster labels\ntsne_df = pd.DataFrame({\n    't-SNE1': tsne_results[:, 0],\n    't-SNE2': tsne_results[:, 1],\n    't-SNE3': tsne_results[:, 2],\n    'Cluster': labels\n})\n\n# Plotting\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Cluster'], cmap='viridis', s=50)\nplt.title('t-SNE Plot of Building Energy Consumption Clusters')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.legend(handles=scatter.legend_elements()[0], labels=set(labels))\nplt.colorbar(scatter, label='Cluster Label')\nplt.show()\n\n\nimport plotly.express as px\n\n# Plotting with Plotly\nplt.figure(figsize=(15,15))\nfig = px.scatter_3d(tsne_df, x='t-SNE1', y='t-SNE2', z='t-SNE3', color='Cluster',\n                    labels={'t-SNE1': 't-SNE Dimension 1', 't-SNE2': 't-SNE Dimension 2', 't-SNE3': 't-SNE Dimension 3'},\n                    color_continuous_scale='Viridis')\nfig.show()\n\n\n\n\n\n\n\n\n                                                \n\n\n&lt;Figure size 1500x1500 with 0 Axes&gt;\n\n\n\n# Apply UMAP for 2D visualization\numap_2d = umap.UMAP(n_components=2, random_state=42)\numap_results_2d = umap_2d.fit_transform(data_scaled_bis)\n\n# Apply UMAP for 3D visualization\numap_3d = umap.UMAP(n_components=3, random_state=42)\numap_results_3d = umap_3d.fit_transform(data_scaled_bis)\n\n# Create DataFrames with UMAP results and K-means cluster labels\numap_df_2d = pd.DataFrame({\n    'UMAP1': umap_results_2d[:, 0],\n    'UMAP2': umap_results_2d[:, 1],\n    'Cluster': labels\n})\n\numap_df_3d = pd.DataFrame({\n    'UMAP1': umap_results_3d[:, 0],\n    'UMAP2': umap_results_3d[:, 1],\n    'UMAP3': umap_results_3d[:, 2],\n    'Cluster': labels\n})\n\n# 2D Plotting with Matplotlib\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(umap_df_2d['UMAP1'], umap_df_2d['UMAP2'], c=umap_df_2d['Cluster'], cmap='viridis', s=50)\nplt.title('UMAP 2D Plot of K-means Clusters')\nplt.xlabel('UMAP Dimension 1')\nplt.ylabel('UMAP Dimension 2')\nplt.colorbar(scatter, label='Cluster Label')\nplt.grid()\nplt.show()\n\n# 3D Plotting with Plotly\nfig = px.scatter_3d(umap_df_3d, x='UMAP1', y='UMAP2', z='UMAP3', color='Cluster',\n                    title='UMAP 3D projection of the data',\n                    labels={'UMAP1': 'UMAP Dimension 1', 'UMAP2': 'UMAP Dimension 2', 'UMAP3': 'UMAP Dimension 3'},\n                    color_continuous_scale='Viridis')\nfig.show()\n\n/home/vincent/anaconda3/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n/home/vincent/anaconda3/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning:\n\nn_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\nVerify clustering by checking the timeseries\n\n# First cluster\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 15})\n\nstart = 10\nstart_week = 168*15\nweeklength = 168*4\n\nhours = np.arange(weeklength)\ndays = hours/24\n\n#plt.figure(figsize=(12,7))\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(0.0, 0.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(-2.5, 2.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(-2.5, -2.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.ylim([0,11])\n\nplt.figure(figsize=(12,7))\nplt.plot(days, df_pivoted.loc[cluster_0[7]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_0[11]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_0[111]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.ylim([0,11])\nplt.ylabel('Consumption (kWh)')\nplt.xlabel('Time (days)')\n\nText(0.5, 0, 'Time (days)')\n\n\n\n\n\n\n\n\n\n\n# Second cluster\n\nstart = 10\nstart_week = 168*22\nweeklength = 168*4\n\nhours = np.arange(weeklength)\ndays = hours/24\n\n#plt.figure(figsize=(12,7))\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(14.0, 3.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(15.0, 2.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(13.5, 1.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.ylim([0,11])\n\nplt.figure(figsize=(12,7))\nplt.plot(days, df_pivoted.loc[cluster_1[65]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_1[16]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_1[25]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.ylim([0,11])\nplt.ylabel('Consumption (kWh)')\nplt.xlabel('Time (days)')\n\nText(0.5, 0, 'Time (days)')\n\n\n\n\n\n\n\n\n\n\n# Third cluster\n\nstart = 10\nstart_week = 168*25\nweeklength = 168*4\n\nhours = np.arange(weeklength)\ndays = hours/24\n\n#plt.figure(figsize=(12,7))\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(0.0, 9.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(1.0, 9.5, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.plot(df_pivoted.loc[revert_umap_projection_2D(0.25, 9.0, umap_df_2d)][1+start_week:start_week+weeklength+1].values, alpha=0.5)\n#plt.ylim([0,11])\n\nplt.figure(figsize=(12,7))\nplt.plot(days, df_pivoted.loc[cluster_2[7]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_2[10]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.plot(days, df_pivoted.loc[cluster_2[111]][1+start_week:start_week+weeklength+1].values, alpha=0.5)\nplt.ylim([0,11])\nplt.ylabel('Consumption (kWh)')\nplt.xlabel('Time (days)')\n\nText(0.5, 0, 'Time (days)')\n\n\n\n\n\n\n\n\n\n\n\nSave the clusters\n\n# Get the rows of the initial dataframe for each cluster\n\numap_df_2d = pd.DataFrame({\n    'UMAP1': umap_results_2d[:, 0],\n    'UMAP2': umap_results_2d[:, 1],\n    'Cluster': labels\n})\n\n\ncluster_0 = []\ncluster_1 = []\ncluster_2 = []\n\n\nfor i,j,k in tqdm(zip(umap_results_2d[:,0], umap_results_2d[:,1], labels)):\n    \n    index = revert_umap_projection_2D(i, j, umap_df_2d)\n    \n    if(k==0):\n        cluster_0.append(index)\n        \n    elif(k==1):\n        cluster_1.append(index)\n        \n    else:\n        cluster_2.append(index)\n\n        \n# Print the number of samples for each cluster\nprint(len(cluster_0), len(cluster_1), len(cluster_2))\n\n\n\n\n2973 457 209\n\n\n\nnp.save(path_to_folder+'TRAIL24/centralized experiments/cluster_0.npy', np.array(cluster_0))\n\nnp.save(path_to_folder+'TRAIL24/centralized experiments/cluster_1.npy', np.array(cluster_1))\n\nnp.save(path_to_folder+'TRAIL24/centralized experiments/cluster_2.npy', np.array(cluster_2))\n\n\n\nFeature importance analysis using XGBoost classifier\n\nX_train, X_test, y_train, y_test = train_test_split(data_scaled_bis, labels, test_size=0.2, random_state=25)\n\nmodel = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1)\n#model = XGBRegressor(n_estimators=500, max_depth=5, eta=0.05)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nAccuracy: 0.9959\n\n\n\nlabels_plots = ['Mean', 'Std Dev', 'Skewness', 'Kurtosis', 'Energy', 'Periodicity', \n                'Trend', 'Seasonality', 'Stationarity', 'Autocor', 'Partial_Autocor']\n\nlabels_plots_bis = np.array(labels_plots)[indexes]\n\n\nfeature_importance = model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\nfig = plt.figure(figsize=(12, 6))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), np.array(labels_plots_bis)[sorted_idx])\nplt.title('Feature Importance')\n\nText(0.5, 1.0, 'Feature Importance')",
    "crumbs": [
      "Experiments",
      "Clustering"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TRAIL24",
    "section": "",
    "text": "flex is a comprehensive toolbox designed to facilitate secure and privacy-preserving federated learning in edge computing environments. With a focus on robust security measures, efficient data handling, and seamless integration, PASTEIS empowers developers and researchers to implement and manage secure federated learning workflows with ease."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "TRAIL24",
    "section": "Features",
    "text": "Features\n\nPrivacy Assurance: Implements state-of-the-art privacy techniques such as differential privacy and secure multi-party computation to protect sensitive data.\nSecure Federated Learning: Provides tools for setting up and managing federated learning models, ensuring data security across distributed edge devices.\nEdge Compatibility: Optimized for deployment in edge computing scenarios, handling data heterogeneity and device variability.\nScalability: Supports scalable solutions for federated learning, accommodating a large number of edge devices.\nEase of Use: Intuitive APIs and extensive documentation to streamline the development process."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "TRAIL24",
    "section": "Install",
    "text": "Install\npip install flex"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "TRAIL24",
    "section": "How to use",
    "text": "How to use"
  },
  {
    "objectID": "experiment.preprocess.html",
    "href": "experiment.preprocess.html",
    "title": "Preprocess",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\nfrom TRAIL24.data.preprocess import *\n\n\nfile_path3 = '/kaggle/input/all-data/residential_all.pkl'\ndf3 = pd.read_pickle(file_path3)\ndf3[\"ID\"] = df3[\"ID\"].astype(\"category\")\ndf3[\"time_code\"] = df3[\"time_code\"].astype(\"uint16\")\n\n\ndf3 = df3.set_index([\"date_time\",\"ID\"])\ndf3 = df3.groupby('ID', group_keys=False, observed=True).apply(resample_building_data)\ndf3=df3.reset_index(level=['ID',\"date_time\"])\n\n\n# Generate the range of date_time values\nstart_time = pd.Timestamp('2009-07-14 00:00:00')\nend_time = pd.Timestamp('2011-01-01 00:00:00')\ndate_range = pd.date_range(start=start_time, end=end_time, freq='h')\n\n# Pivot the dataset\ndf_pivoted = df3.pivot(index='ID', columns='date_time', values='consumption').reset_index()\n\n# Ensure columns are sorted by date_time\n# df_pivoted = df_pivoted.sort_index(axis=1)\n\n# Optional: Rename the columns to make them more readable\ndf_pivoted.columns.name = None\ndf_pivoted.columns = ['ID'] + [date.strftime('%Y-%m-%d %H:%M:%S') for date in date_range]\n\n\n# Assuming df_pivoted is your pivoted DataFrame with 'ID' as the first column\n# Create a list to store the aggregation results\naggregation_results = []\n\nfor i, row in df_pivoted.iterrows():\n    building_data = row[1:].astype(float)  # Skip the first column (ID)\n    building_series = pd.Series(building_data.values, index=pd.date_range(start='2009-07-14 00:00:00', periods=len(building_data), freq='h'))\n    \n    # Perform the aggregations\n    weekday_avg = weekday_average(building_series)\n    segment_avg = day_segment_average(building_series)\n    total_energy = total_energy_used(building_series)\n    avg_energy = average_energy_used(building_series)\n    we_bd_avg = weekend_businessday_avg(building_series)\n    \n    # Combine all the aggregation results for the current building\n    aggregation_result = np.concatenate([weekday_avg, segment_avg, [total_energy], avg_energy, we_bd_avg])\n    aggregation_results.append(aggregation_result)\n\n# Define the columns for the aggregation results\naggregation_columns = [\n    'avg_mon', 'avg_tue', 'avg_wed', 'avg_thu', 'avg_fri', 'avg_sat', 'avg_sun',\n    'avg_early_morning', 'avg_morning', 'avg_early_afternoon', 'avg_late_afternoon', 'avg_night',\n    'total_energy_used',\n    'hourly_avg_energy', 'daily_avg_energy', 'weekly_avg_energy', 'monthly_avg_energy',\n    'weekend_avg_energy', 'business_day_avg_energy'\n]\n\n# Convert the aggregation results to a DataFrame\ndf_aggregations = pd.DataFrame(aggregation_results, columns=aggregation_columns)\ndf_aggregations[\"business_day_avg_energy\"] = df_aggregations[\"avg_mon\"]+df_aggregations[\"avg_tue\"]+df_aggregations[\"avg_wed\"]+df_aggregations[\"avg_thu\"]+df_aggregations[\"avg_fri\"]\ndf_aggregations[\"weekend_avg_energy\"] = df_aggregations[\"avg_sat\"]+df_aggregations[\"avg_sun\"]\n\n# Concatenate the original pivoted dataframe with the aggregations dataframe\ndf_final = pd.concat([df_pivoted.reset_index(drop=True), df_aggregations], axis=1)\n\n# Display the final dataframe\nlen(df_final)",
    "crumbs": [
      "Experiments",
      "Preprocess"
    ]
  },
  {
    "objectID": "experiment.compression.html",
    "href": "experiment.compression.html",
    "title": "Compression",
    "section": "",
    "text": "#from TRAIL24.compression.pruning import *\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#from torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\nfrom torch.utils.data import DataLoader, Dataset\nimport time\n#from memory_profiler import memory_usage\nimport resource\nimport pickle\nimport gc\nimport warnings\nfrom tqdm.notebook import tqdm\n\nfrom torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nwith open('../data/df_cluster_0.pkl', 'rb') as f:\n    df = pickle.load(f)\n\n\n# In[5]:\n\nprint('OKKKK')\n\nnum_rows = len(df)\n\n# Calculate the halfway point\nhalfway_point = num_rows // 4\n\n# Select the first half of the rows\ndf = df.iloc[:halfway_point]\n\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Function to optimize data types\ndef optimize_data_types(df):\n    # Optimize numeric columns\n    for col in df.select_dtypes(include=['int']).columns:\n        df[col] = df[col].astype('int32')\n    \n    for col in df.select_dtypes(include=['float']).columns:\n        df[col] = df[col].astype('float32')\n\n    # Optimize object columns\n    for col in df.select_dtypes(include=['object']).columns:\n        num_unique_values = len(df[col].unique())\n        num_total_values = len(df[col])\n        if num_unique_values / num_total_values &lt; 0.5:\n            df[col] = df[col].astype('category')\n    \n    return df\n\n# Load the data and remove the first and last columns\n#df = pd.read_csv('df_labeled0.csv')\ndf = df.iloc[:, 1:-1]\n\n# Optimize data types\ndf = optimize_data_types(df)\n\n# Define the window size for lagged inputs\nwindow_size = 40  # Replace N with your desired window size\n\n# Function to process each chunk\ndef process_chunk(chunk, window_size):\n    data = []\n    for i, row in tqdm(chunk.iterrows()):\n        values = row.values\n        for t in range(window_size, len(values)):\n            lagged_inputs = values[t-window_size:t]\n            target_value = values[t]\n\n            # Calculate realistic datetime values using the index\n            base_date = pd.Timestamp('2009-07-14 00:00:00')\n            timestamp = base_date + pd.Timedelta(hours=t)\n            year = timestamp.year\n            month = timestamp.month\n            day = timestamp.day\n            hour = timestamp.hour\n\n            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n    return data\n\n# Create an empty DataFrame to store results\ncolumns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\ndf_transformed = pd.DataFrame(columns=columns)\n\n# Process the data in chunks\nchunk_size = 100  # Adjust based on available memory\nfor start_row in tqdm(range(0, df.shape[0], chunk_size)):\n    chunk = df.iloc[start_row:start_row + chunk_size]\n    data = process_chunk(chunk, window_size)\n    df_chunk_transformed = pd.DataFrame(data, columns=columns)\n    df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n\n# Make sure year, month, day, and hour are integers\ndf_transformed['year'] = df_transformed['year'].astype(int)\ndf_transformed['month'] = df_transformed['month'].astype(int)\ndf_transformed['day'] = df_transformed['day'].astype(int)\ndf_transformed['hour'] = df_transformed['hour'].astype(int)\n\n# Create datetime column\ntry:\n    df_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\n    print(\"Date conversion successful.\")\nexcept Exception as e:\n    print(\"Error during date conversion:\", e)\n\n# Apply OrdinalEncoder to month, day, hour columns\nordinal_encoder = OrdinalEncoder()\ndf_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n\nprint(\"Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\")\n\n\n\n\nprint(df_transformed[['year', 'month', 'day', 'hour']].dtypes)\n\n\n# In[9]:\n\n\nprint(df_transformed['month'].unique())  # Should be within 1-12\nprint(df_transformed['day'].unique())    # Should be within 1-31\nprint(df_transformed['hour'].unique())   # Should be within 0-23\n\n\n# In[10]:\n\n\n# Convert the date columns to datetime\ndf_transformed['year1'] = df_transformed['year']\ndf_transformed['month1'] = df_transformed['month']\ndf_transformed['day1'] = df_transformed['day']\ndf_transformed['hour1'] = df_transformed['hour']\ndf_transformed[\"target1\"] = df_transformed[\"target\"]\n\n# Define the date ranges for training and testing\ntrain_start_date = '2009-07-14'\ntrain_end_date = '2010-12-15'\ntest_start_date = '2010-12-15'\ntest_end_date = '2011-01-01'\n\n# Convert the date columns to datetime\ndf_transformed['year'] = df_transformed['year'].astype(int)\ndf_transformed['month'] = df_transformed['month'].astype(int)+1\ndf_transformed['day'] = df_transformed['day'].astype(int)+1\ndf_transformed['hour'] = df_transformed['hour'].astype(int)+1\n\n# Create a datetime column\ndf_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\ndf_sample = df_transformed.loc[:12825]\n\n# Set the datetime column as the index\ndf_transformed.set_index('date_time', inplace=True)\ndf_sample.set_index('date_time', inplace=True)\n\n# Drop the irrelevant columns\ndf_transformed.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\ndf_sample.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n\ndf_transformed = df_transformed.sort_index()\ndf_sample = df_sample.sort_index()\n\n# Split the data into training and testing sets\ntrain = df_transformed.loc[train_start_date:train_end_date]\n\ntest = df_transformed.loc[test_start_date:test_end_date]\n\nscaler = StandardScaler()\n\n# Fit the scaler on the first 40 columns of the training data\nscaler.fit(train.iloc[:, :40])\n\n# Transform the first 40 columns of both training and validation data\ntrain.iloc[:, :40] = scaler.transform(train.iloc[:, :40])\ntest.iloc[:, :40] = scaler.transform(test.iloc[:, :40])\n\n# Create sample data for the first building\nsample = df_sample.loc[test_start_date:test_end_date]\n\nsample.iloc[:, :40] = scaler.transform(sample.iloc[:, :40])\n\ndel df_transformed, df\ngc.collect()\n\nOKKKK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate conversion successful.\nTransformation complete. The new dataset is saved as 'transformed_dataset.csv'.\nyear       int64\nmonth    float64\nday      float64\nhour     float64\ndtype: object\n[ 6.  7.  8.  9. 10. 11.  0.  1.  2.  3.  4.  5.]\n[14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30.  0.\n  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n[16. 17. 18. 19. 20. 21. 22. 23.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9.\n 10. 11. 12. 13. 14. 15.]\n\n\n\n---------------------------------------------------------------------------\nSettingWithCopyError                      Traceback (most recent call last)\n/tmp/ipykernel_2151806/3735520156.py in ?()\n--&gt; 148 with open('../data/df_cluster_0.pkl', 'rb') as f:\n    149     df = pickle.load(f)\n    150 \n    151 \n\n~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/frame.py in ?(self, labels, axis, index, columns, level, inplace, errors)\n   5577                 weight  250.0   150.0\n   5578         falcon  speed   320.0   250.0\n   5579                 weight  1.0     0.8\n   5580         \"\"\"\n-&gt; 5581         return super().drop(\n   5582             labels=labels,\n   5583             axis=axis,\n   5584             index=index,\n\n~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py in ?(self, labels, axis, index, columns, level, inplace, errors)\n   4787             if labels is not None:\n   4788                 obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n   4789 \n   4790         if inplace:\n-&gt; 4791             self._update_inplace(obj)\n   4792             return None\n   4793         else:\n   4794             return obj\n\n~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py in ?(self, result, verify_is_copy)\n   4895         # decision that we may revisit in the future.\n   4896         self._reset_cache()\n   4897         self._clear_item_cache()\n   4898         self._mgr = result._mgr\n-&gt; 4899         self._maybe_update_cacher(verify_is_copy=verify_is_copy, inplace=True)\n\n~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py in ?(self, clear, verify_is_copy, inplace)\n   4012         if using_copy_on_write():\n   4013             return\n   4014 \n   4015         if verify_is_copy:\n-&gt; 4016             self._check_setitem_copy(t=\"referent\")\n   4017 \n   4018         if clear:\n   4019             self._clear_item_cache()\n\n~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py in ?(self, t, force)\n   4469                 \"indexing.html#returning-a-view-versus-a-copy\"\n   4470             )\n   4471 \n   4472         if value == \"raise\":\n-&gt; 4473             raise SettingWithCopyError(t)\n   4474         if value == \"warn\":\n   4475             warnings.warn(t, SettingWithCopyWarning, stacklevel=find_stack_level())\n\nSettingWithCopyError: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n# Instantiate a StandardScaler\nscaler = StandardScaler()\n\n# Standardize only the first 40 columns\nsample.iloc[:, :40] = scaler.fit_transform(sample.iloc[:, :40])\n\n# Display the updated DataFrame\nprint(df)\n\n     2009-07-14 00:00:00  2009-07-14 01:00:00  2009-07-14 02:00:00  \\\n1                  0.692                0.761                0.725   \n2                  1.310                2.360                1.693   \n3                  0.177                0.324                0.317   \n4                  0.048                0.183                0.101   \n5                  0.614                1.142                1.139   \n..                   ...                  ...                  ...   \n908                0.128                0.297                0.284   \n909                0.229                0.402                0.348   \n912                0.626                0.691                0.375   \n913                0.179                0.183                0.178   \n915                0.448                0.372                0.292   \n\n     2009-07-14 03:00:00  2009-07-14 04:00:00  2009-07-14 05:00:00  \\\n1                  0.546                0.729                0.755   \n2                  1.738                0.833                0.789   \n3                  0.311                0.305                0.304   \n4                  0.175                1.062                0.322   \n5                  0.836                0.805                0.826   \n..                   ...                  ...                  ...   \n908                0.663                0.250                0.183   \n909                0.297                0.393                0.339   \n912                0.491                0.386                0.252   \n913                0.199                0.183                0.172   \n915                0.292                0.283                0.287   \n\n     2009-07-14 06:00:00  2009-07-14 07:00:00  2009-07-14 08:00:00  \\\n1                  0.773                0.743                0.911   \n2                  0.709                0.756                1.397   \n3                  0.303                0.300                1.213   \n4                  0.151                0.241                0.157   \n5                  0.511                0.707                0.447   \n..                   ...                  ...                  ...   \n908                0.321                0.184                0.219   \n909                0.306                0.382                0.564   \n912                0.508                0.539                0.875   \n913                0.172                0.181                0.449   \n915                0.289                3.976                0.354   \n\n     2009-07-14 09:00:00  ...  2010-12-31 14:00:00  2010-12-31 15:00:00  \\\n1                  0.579  ...                0.809                1.901   \n2                  4.208  ...                0.782                0.961   \n3                  0.730  ...                1.998                1.540   \n4                  0.117  ...                0.744                6.229   \n5                  0.296  ...                3.103                2.959   \n..                   ...  ...                  ...                  ...   \n908                0.621  ...                0.125                0.149   \n909                0.499  ...                0.930                0.755   \n912                0.673  ...                1.560                1.561   \n913                0.390  ...                0.579                1.443   \n915                0.276  ...                5.138                1.230   \n\n     2010-12-31 16:00:00  2010-12-31 17:00:00  2010-12-31 18:00:00  \\\n1                  2.744                4.013                3.007   \n2                  2.522                5.785                6.388   \n3                  4.620                5.607                6.813   \n4                  5.182                6.428                7.117   \n5                  2.676                5.144                6.581   \n..                   ...                  ...                  ...   \n908                0.119                0.194                0.197   \n909                2.359                1.301                2.171   \n912                4.066                3.383                1.229   \n913                0.646                1.480                0.585   \n915                1.054                1.582                1.952   \n\n     2010-12-31 19:00:00  2010-12-31 20:00:00  2010-12-31 21:00:00  \\\n1                  2.166                1.617                1.636   \n2                  3.271                2.947                3.106   \n3                  4.491                1.505                1.457   \n4                  6.624                3.757                2.395   \n5                  6.868                2.603                2.259   \n..                   ...                  ...                  ...   \n908                0.319                0.293                0.245   \n909                4.659                5.219                1.733   \n912                1.201                3.447                1.886   \n913                0.732                0.589                0.582   \n915                1.666                1.675                0.877   \n\n     2010-12-31 22:00:00  2010-12-31 23:00:00  \n1                  1.700                1.674  \n2                  3.241                2.846  \n3                  1.089                1.124  \n4                  2.286                2.213  \n5                  1.688                1.628  \n..                   ...                  ...  \n908                0.166                0.120  \n909                1.957                1.743  \n912                2.004                1.569  \n913                0.591                1.479  \n915                0.876                0.879  \n\n[743 rows x 12864 columns]\n\n\n\ndf_transformed.iloc[:, :40].max()\n\nlag_1     24.517\nlag_2     24.517\nlag_3     24.517\nlag_4     24.517\nlag_5     24.517\nlag_6     24.517\nlag_7     24.517\nlag_8     24.517\nlag_9     24.517\nlag_10    24.517\nlag_11    24.517\nlag_12    24.517\nlag_13    24.517\nlag_14    24.517\nlag_15    24.517\nlag_16    24.517\nlag_17    24.517\nlag_18    24.517\nlag_19    24.517\nlag_20    24.517\nlag_21    24.517\nlag_22    24.517\nlag_23    24.517\nlag_24    24.517\nlag_25    24.517\nlag_26    24.517\nlag_27    24.517\nlag_28    24.517\nlag_29    24.517\nlag_30    24.517\nlag_31    24.517\nlag_32    24.517\nlag_33    24.517\nlag_34    24.517\nlag_35    24.517\nlag_36    24.517\nlag_37    24.517\nlag_38    24.517\nlag_39    24.517\nlag_40    24.517\ndtype: float32\n\n\n\n\n\n\n\n\n\n\n\n\nlag_1\nlag_2\nlag_3\nlag_4\nlag_5\nlag_6\nlag_7\nlag_8\nlag_9\nlag_10\n...\nlag_36\nlag_37\nlag_38\nlag_39\nlag_40\nyear1\nmonth1\nday1\nhour1\ntarget1\n\n\ndate_time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2010-12-15 00:00:00\n0.057708\n0.070939\n0.079591\n0.056690\n0.192157\n0.119895\n0.046106\n0.039592\n0.043052\n0.093331\n...\n0.350626\n0.184117\n0.315105\n0.208035\n0.301671\n2010\n11.0\n13.0\n23.0\n3.437\n\n\n2010-12-15 01:00:00\n0.067060\n0.075238\n0.053590\n0.181650\n0.113339\n0.043584\n0.037427\n0.040698\n0.088227\n0.244476\n...\n0.174049\n0.297874\n0.196659\n0.285174\n0.330683\n2010\n11.0\n14.0\n0.0\n1.874\n\n\n2010-12-15 02:00:00\n0.074206\n0.052855\n0.179158\n0.111784\n0.042986\n0.036913\n0.040140\n0.087017\n0.241123\n0.234385\n...\n0.293788\n0.193961\n0.281262\n0.326147\n0.177829\n2010\n11.0\n14.0\n1.0\n1.832\n\n\n2010-12-15 03:00:00\n0.052214\n0.176984\n0.110427\n0.042465\n0.036465\n0.039653\n0.085961\n0.238197\n0.231541\n0.327908\n...\n0.191608\n0.277850\n0.322189\n0.175671\n0.171734\n2010\n11.0\n14.0\n2.0\n0.774\n\n\n2010-12-15 04:00:00\n0.176760\n0.110288\n0.042411\n0.036419\n0.039602\n0.085852\n0.237895\n0.231248\n0.327492\n0.157567\n...\n0.277498\n0.321781\n0.175449\n0.171517\n0.072464\n2010\n11.0\n14.0\n3.0\n0.723\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2010-12-31 20:00:00\n0.064250\n0.046326\n0.036749\n0.037895\n0.042806\n0.088559\n0.088886\n0.126127\n0.183747\n0.178836\n...\n0.066215\n0.155592\n0.224589\n0.328453\n0.246115\n2010\n11.0\n30.0\n19.0\n2.166\n\n\n2010-12-31 21:00:00\n0.045706\n0.036258\n0.037388\n0.042234\n0.087374\n0.087697\n0.124440\n0.181289\n0.176444\n0.216901\n...\n0.153510\n0.221585\n0.324060\n0.242823\n0.174910\n2010\n11.0\n30.0\n20.0\n1.617\n\n\n2010-12-31 22:00:00\n0.035990\n0.037112\n0.041921\n0.086728\n0.087048\n0.123519\n0.179948\n0.175139\n0.215297\n0.088972\n...\n0.219946\n0.321662\n0.241026\n0.173616\n0.129611\n2010\n11.0\n30.0\n21.0\n1.636\n\n\n2010-12-31 23:00:00\n0.036820\n0.041592\n0.086046\n0.086364\n0.122549\n0.178534\n0.173763\n0.213605\n0.088273\n0.111256\n...\n0.319135\n0.239133\n0.172252\n0.128592\n0.130103\n2010\n11.0\n30.0\n22.0\n1.700\n\n\n2011-01-01 00:00:00\n0.041244\n0.085327\n0.085643\n0.121525\n0.177043\n0.172311\n0.211820\n0.087536\n0.110326\n0.104806\n...\n0.237135\n0.170813\n0.127518\n0.129016\n0.134064\n2010\n11.0\n30.0\n23.0\n1.674\n\n\n\n\n409 rows × 45 columns\n\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].astype(\"float32\")\n        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n        year = torch.FloatTensor([row[-5]]).to(torch.int)\n        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is expected to be 0-11\n        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is expected to be 0-30\n        hour = torch.FloatTensor([row[-2]]).to(torch.int)      # Hour is already 0-indexed (0-23)\n        label = torch.FloatTensor([row[-1]])  # The last column\n        return features, year, month, day, hour, label\n\n    def __len__(self):\n        return len(self.dataframe)\n\ndef create_data_loaders(data, chunk_size, batch_size):\n    chunks = np.array_split(data, len(data) // chunk_size)\n    data_loaders = []\n    for chunk in chunks:\n        dataset = CustomDataset(chunk)\n        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        data_loaders.append(data_loader)\n    return data_loaders\n\n\n# In[13]:\n\n\ndef train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cuda\")):\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n    for epoch in tqdm(range(n_epochs)):\n        epoch_loss = 0\n        counter = 0\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in tqdm(data_loader):\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                loss = criterion(outputs, labels)\n                epoch_loss += loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        print(f\"epoch {epoch}\")\n        scheduler.step(epoch_loss / len(data_loaders))\n        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n        collected = gc.collect()\n    return net\n\ndef test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    net.eval()\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():  # to not reserve a memory space for gradients\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in tqdm(data_loader):\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                outputs = outputs.squeeze()\n                labels = labels.squeeze()\n\n                mse(outputs, labels)\n                smape(outputs, labels)\n                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n    if return_data:\n        return torch.cat(list_outputs), torch.cat(list_targets), test_mse, test_smape\n\n\n# Define a function to create the model for memory profiling\ndef create_model():\n    model = CombinedModel(nbits_params, embedding_dim, final_hidden)\n    return model\n\ndef get_memory_usage():\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n\nclass TrendBlock(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers):\n        super(TrendBlock, self).__init__()\n        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n        self.backcast_fc = nn.Linear(num_hidden, input_size)\n        self.forecast_fc = nn.Linear(num_hidden, output_size)\n\n    def forward(self, x):\n        for layer in self.fc:\n            x = torch.relu(layer(x))\n        backcast = self.backcast_fc(x)\n        forecast = self.forecast_fc(x)\n        return backcast, forecast\n\nclass SeasonalityBlock(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers):\n        super(SeasonalityBlock, self).__init__()\n        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n        self.backcast_fc = nn.Linear(num_hidden, input_size)\n        self.forecast_fc = nn.Linear(num_hidden, output_size)\n\n    def forward(self, x):\n        for layer in self.fc:\n            x = torch.relu(layer(x))\n        backcast = self.backcast_fc(x)\n        forecast = self.forecast_fc(x)\n        return backcast, forecast\n\nclass NBEATS(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers, num_blocks):\n        super(NBEATS, self).__init__()\n        self.trend_blocks = nn.ModuleList([TrendBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n        self.seasonality_blocks = nn.ModuleList([SeasonalityBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n\n    def forward(self, x):\n        backcast, forecast = x, 0\n        for block in self.trend_blocks:\n            backcast, block_forecast = block(x)\n            x = x - backcast\n            forecast = forecast + block_forecast\n        for block in self.seasonality_blocks:\n            backcast, block_forecast = block(x)\n            x = x - backcast\n            forecast = forecast + block_forecast\n        return forecast\n\nclass EmbeddingNetwork(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super(EmbeddingNetwork, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, x):\n        return self.embedding(x)\n\nclass YearNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super(YearNetwork, self).__init__()\n        self.hidden = nn.Linear(input_dim, 25)\n        self.output = nn.Linear(25, num_hidden)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden(x))\n        return self.output(x)\n\nclass CombinedModel(nn.Module):\n    def __init__(self, nbits_params, embedding_dim, final_hidden):\n        super(CombinedModel, self).__init__()\n        self.nbeats = NBEATS(**nbits_params)\n        self.month_net = EmbeddingNetwork(12, embedding_dim)  # Months from 1 to 12\n        self.day_net = EmbeddingNetwork(31, embedding_dim)    # Days from 1 to 31\n        self.hour_net = EmbeddingNetwork(24, embedding_dim)   # Hours from 0 to 23\n        self.year_net = YearNetwork(1, 10)\n        self.final_layer = nn.Sequential(\n            #nn.Linear(embedding_dim * 3 + nbits_params['output_size'] + 10, final_hidden),\n            nn.Linear(embedding_dim * 3 + nbits_params['output_size'], final_hidden),\n            nn.ReLU(),\n            nn.Linear(final_hidden, 1)\n        )\n\n    def forward(self, x, month, day, hour, year):\n        ts_output = self.nbeats(x)\n        #print('TS', ts_output)\n        month_output = self.month_net(month.long()).squeeze(1)\n        #print('Month', month_output)\n        day_output = self.day_net(day.long()).squeeze(1)\n        #print('Day', day_output)\n        hour_output = self.hour_net(hour.long()).squeeze(1)\n        #print('Hour', hour_output)\n        \n        #print('Year', year_output)\n        #combined_output = torch.cat((ts_output, month_output, day_output, hour_output, year_output), dim=1)\n        combined_output = torch.cat((ts_output, month_output, day_output, hour_output), dim=1)\n        final_output = self.final_layer(combined_output)\n        return final_output\n\n\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 256\n\nnbits_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\n\nnet = create_model()\n\n\ntrained_model = torch.load('trained_model.pth')\n\n\ntrained_model\n\nCombinedModel(\n  (nbeats): NBEATS(\n    (trend_blocks): ModuleList(\n      (0-5): 6 x TrendBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n    (seasonality_blocks): ModuleList(\n      (0-5): 6 x SeasonalityBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n  )\n  (month_net): EmbeddingNetwork(\n    (embedding): Embedding(12, 10)\n  )\n  (day_net): EmbeddingNetwork(\n    (embedding): Embedding(31, 10)\n  )\n  (hour_net): EmbeddingNetwork(\n    (embedding): Embedding(24, 10)\n  )\n  (year_net): YearNetwork(\n    (hidden): Linear(in_features=1, out_features=25, bias=True)\n    (output): Linear(in_features=25, out_features=10, bias=True)\n  )\n  (final_layer): Sequential(\n    (0): Linear(in_features=31, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\nlr = 0.0005\nn_epochs = 10\nwindow_size = 40\nchunk_size = 122880\nbatch_size = 1024*12*5\n\n\n# In[15]:\n\n\n# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\ntrain_loaders = create_data_loaders(train, chunk_size, batch_size)\ntest_loaders = create_data_loaders(test, chunk_size, batch_size)\nsample_loaders = create_data_loaders(sample, 100, 1)\n\ndevice = \"cuda\"\nnet.to(device)\n\nCombinedModel(\n  (nbeats): NBEATS(\n    (trend_blocks): ModuleList(\n      (0-5): 6 x TrendBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n    (seasonality_blocks): ModuleList(\n      (0-5): 6 x SeasonalityBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=512, bias=True)\n          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n        )\n        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n  )\n  (month_net): EmbeddingNetwork(\n    (embedding): Embedding(12, 10)\n  )\n  (day_net): EmbeddingNetwork(\n    (embedding): Embedding(31, 10)\n  )\n  (hour_net): EmbeddingNetwork(\n    (embedding): Embedding(24, 10)\n  )\n  (year_net): YearNetwork(\n    (hidden): Linear(in_features=1, out_features=25, bias=True)\n    (output): Linear(in_features=25, out_features=10, bias=True)\n  )\n  (final_layer): Sequential(\n    (0): Linear(in_features=31, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n*_, test_mse, test_smape = test_function(trained_model, test_loaders, None, None, torch.device(device), return_data=True)\n\n\n\n\n\n\n\nTest MSE: 1.037216067314148 , SMAPE 0.4622192978858948\n\n\n\ntest_mse\n\ntensor(1.0372, device='cuda:0')\n\n\n\ntest_smape\n\ntensor(0.4622, device='cuda:0')\n\n\n\nimport torch_pruning as tp\nfrom torch_pruning.pruner import function\nfrom fasterai.core.all import *\n\nimport onnx\nimport onnxruntime as ort\nfrom onnxruntime import quantization\n\nimport numpy as np\nimport pickle\nfrom itertools import cycle\nfrom fastcore.basics import store_attr, listify, true\nfrom fasterbench.benchmark import *\n\ndef get_ignored_layers(model):\n    ignored_layers = []\n\n    # Check and process trend blocks if they exist\n    if hasattr(model.nbeats, 'trend_blocks'):\n        for block in model.nbeats.trend_blocks:\n            if hasattr(block, 'backcast_fc'):\n                ignored_layers.append(block.backcast_fc)\n            if hasattr(block, 'forecast_fc'):\n                ignored_layers.append(block.forecast_fc)\n\n    # Check and process seasonality blocks if they exist\n    if hasattr(model.nbeats, 'seasonality_blocks'):\n        for block in model.nbeats.seasonality_blocks:\n            if hasattr(block, 'backcast_fc'):\n                ignored_layers.append(block.backcast_fc)\n            if hasattr(block, 'forecast_fc'):\n                ignored_layers.append(block.forecast_fc)\n\n    return ignored_layers\n\ndef adjust_layer_features(layer, pruning_ratio):\n    if hasattr(layer, 'in_features') and hasattr(layer, 'weight'):\n        in_features = layer.in_features\n        layer.in_features = int(in_features * (1-pruning_ratio))\n        local_scores = large_final(layer, 'column')\n        threshold = torch.quantile(local_scores.view(-1), pruning_ratio)\n        mask = local_scores.ge(threshold).to(dtype=local_scores.dtype)\n        ixs = torch.nonzero(mask[0] == 1, as_tuple=True)[0]\n        layer.weight.data = layer.weight[:, ixs]\n\ndef prune_model(model, pruning_ratio, dummy_input):\n    imp = tp.importance.GroupNormImportance(p=2)\n    ignored_layers = get_ignored_layers(model)\n\n    pruner = tp.pruner.MetaPruner(\n        model.nbeats,\n        dummy_input,\n        importance=imp,\n        pruning_ratio=pruning_ratio, \n        ignored_layers=ignored_layers\n    )\n    pruner.step()\n    \n    if hasattr(model.nbeats, 'trend_blocks'):\n        for block in model.nbeats.trend_blocks:\n            for layer in [block.backcast_fc, block.forecast_fc]:\n                if layer is not None and layer.in_features != int(num_hidden*(1-pruning_ratio)):\n                    adjust_layer_features(layer, pruning_ratio)\n\n    if hasattr(model.nbeats, 'seasonality_blocks'):\n        for block in model.nbeats.seasonality_blocks:\n            for layer in [block.backcast_fc, block.forecast_fc]:\n                if layer is not None and layer.in_features != int(num_hidden*(1-pruning_ratio)):\n                    adjust_layer_features(layer, pruning_ratio)\n\n\nnum_parameters = get_num_parameters(trained_model)\ndisk_size = get_model_size(trained_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 num_parameters = get_num_parameters(trained_model)\n      2 disk_size = get_model_size(trained_model)\n      3 print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nNameError: name 'trained_model' is not defined\n\n\n\n\nbatch_size = 5\nnum_features = 40\n\nfeatures = torch.randn(batch_size, num_features)\n\n\ntrained_model = torch.load('trained_model.pth')\n\n\nprune_model(trained_model, 0.15, features.to('cuda'))\n\n\nnum_parameters = get_num_parameters(trained_model)\ndisk_size = get_model_size(trained_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 65.56 MB (disk), 16369401 parameters\n\n\n\n*_, test_mse, test_smape = test_function(trained_model, test_loaders, None, None, torch.device(device), return_data=True)\n\n\n\n\n\n\n\nTest MSE: 1.1977158784866333 , SMAPE 0.4805765748023987\n\n\n\ntest_mse, test_smape\n\n(tensor(1.9425, device='cuda:0'), tensor(0.7663, device='cuda:0'))\n\n\n\nimport torch\nimport torch.nn as nn\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\n\n\ndef script_model(model, dummy_input, path='scripted_model.pt'):\n    scripted_model = torch.jit.trace(model, dummy_input)\n    scripted_model.save(path)\n    return scripted_model\n\n\ndef quantize_onnx(model, dummy_input, onnx_path=\"model.onnx\", quant_onnx_path=\"model_quantized.onnx\"):\n    torch.onnx.export(\n    model,              \n    dummy_input,        \n    onnx_path, \n    input_names=[\"features\", \"year\", \"month\", \"day\", \"hour\"],   \n    output_names=[\"output\"], \n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}, \n    opset_version=11\n    )\n\n    quantize_dynamic(\n        onnx_path,\n        quant_onnx_path,\n        weight_type=QuantType.QUInt8\n    )\n\n\nbatch_size = 5\nnum_features = 40\n\nfeatures = torch.randn(batch_size, num_features)\nmonth = torch.randint(0, 12, (batch_size, 1))      # Random months between 1 and 12\nday = torch.randint(0, 31, (batch_size, 1))        # Random days between 1 and 31\nhour = torch.randint(0, 24, (batch_size, 1))       # Random hours between 0 and 23\nyear = torch.randint(2019, 2021, (batch_size, 1)) \n\nexample_input = features, year, month, day, hour\n\n\ntrained_model\n\nCombinedModel(\n  (nbeats): NBEATS(\n    (trend_blocks): ModuleList(\n      (0-5): 6 x TrendBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=435, bias=True)\n          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n        )\n        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n      )\n    )\n    (seasonality_blocks): ModuleList(\n      (0-5): 6 x SeasonalityBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=435, bias=True)\n          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n        )\n        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n      )\n    )\n  )\n  (month_net): EmbeddingNetwork(\n    (embedding): Embedding(12, 10)\n  )\n  (day_net): EmbeddingNetwork(\n    (embedding): Embedding(31, 10)\n  )\n  (hour_net): EmbeddingNetwork(\n    (embedding): Embedding(24, 10)\n  )\n  (year_net): YearNetwork(\n    (hidden): Linear(in_features=1, out_features=25, bias=True)\n    (output): Linear(in_features=25, out_features=10, bias=True)\n  )\n  (final_layer): Sequential(\n    (0): Linear(in_features=31, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\ntrained_model.to('cpu')(*example_input)\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 trained_model.to('cpu')(*example_input)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[3], line 93, in CombinedModel.forward(self, x, month, day, hour, year)\n     91 ts_output = self.nbeats(x)\n     92 #print('TS', ts_output)\n---&gt; 93 month_output = self.month_net(month.long()).squeeze(1)\n     94 #print('Month', month_output)\n     95 day_output = self.day_net(day.long()).squeeze(1)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nCell In[3], line 63, in EmbeddingNetwork.forward(self, x)\n     62 def forward(self, x):\n---&gt; 63     return self.embedding(x)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163, in Embedding.forward(self, input)\n    162 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 163     return F.embedding(\n    164         input, self.weight, self.padding_idx, self.max_norm,\n    165         self.norm_type, self.scale_grad_by_freq, self.sparse)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/functional.py:2237, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2231     # Note [embedding_renorm set_grad_enabled]\n   2232     # XXX: equivalent to\n   2233     # with torch.no_grad():\n   2234     #   torch.embedding_renorm_\n   2235     # remove once script supports set_grad_enabled\n   2236     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2237 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n\nIndexError: index out of range in self\n\n\n\n\nscripted_model = script_model(trained_model.to('cpu'), example_input)\nquantize_onnx(net, example_input)\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 scripted_model = script_model(trained_model.to('cpu'), example_input)\n      2 quantize_onnx(net, example_input)\n\nCell In[12], line 2, in script_model(model, dummy_input, path)\n      1 def script_model(model, dummy_input, path='scripted_model.pt'):\n----&gt; 2     scripted_model = torch.jit.trace(model, dummy_input)\n      3     scripted_model.save(path)\n      4     return scripted_model\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/jit/_trace.py:806, in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\n    804         else:\n    805             raise RuntimeError(\"example_kwarg_inputs should be a dict\")\n--&gt; 806     return trace_module(\n    807         func,\n    808         {\"forward\": example_inputs},\n    809         None,\n    810         check_trace,\n    811         wrap_check_inputs(check_inputs),\n    812         check_tolerance,\n    813         strict,\n    814         _force_outplace,\n    815         _module_class,\n    816         example_inputs_is_kwarg=isinstance(example_kwarg_inputs, dict),\n    817         _store_inputs=_store_inputs,\n    818     )\n    819 if (\n    820     hasattr(func, \"__self__\")\n    821     and isinstance(func.__self__, torch.nn.Module)\n    822     and func.__name__ == \"forward\"\n    823 ):\n    824     if example_inputs is None:\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/jit/_trace.py:1074, in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\n   1072 else:\n   1073     example_inputs = make_tuple(example_inputs)\n-&gt; 1074     module._c._create_method_from_trace(\n   1075         method_name,\n   1076         func,\n   1077         example_inputs,\n   1078         var_lookup_fn,\n   1079         strict,\n   1080         _force_outplace,\n   1081         argument_names,\n   1082         _store_inputs,\n   1083     )\n   1085 check_trace_method = module._c._get_method(method_name)\n   1087 # Check the trace against new traces created from user-specified inputs\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._slow_forward(self, *input, **kwargs)\n   1499         recording_scopes = False\n   1500 try:\n-&gt; 1501     result = self.forward(*input, **kwargs)\n   1502 finally:\n   1503     if recording_scopes:\n\nCell In[3], line 93, in CombinedModel.forward(self, x, month, day, hour, year)\n     91 ts_output = self.nbeats(x)\n     92 #print('TS', ts_output)\n---&gt; 93 month_output = self.month_net(month.long()).squeeze(1)\n     94 #print('Month', month_output)\n     95 day_output = self.day_net(day.long()).squeeze(1)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._slow_forward(self, *input, **kwargs)\n   1499         recording_scopes = False\n   1500 try:\n-&gt; 1501     result = self.forward(*input, **kwargs)\n   1502 finally:\n   1503     if recording_scopes:\n\nCell In[3], line 63, in EmbeddingNetwork.forward(self, x)\n     62 def forward(self, x):\n---&gt; 63     return self.embedding(x)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._slow_forward(self, *input, **kwargs)\n   1499         recording_scopes = False\n   1500 try:\n-&gt; 1501     result = self.forward(*input, **kwargs)\n   1502 finally:\n   1503     if recording_scopes:\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163, in Embedding.forward(self, input)\n    162 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 163     return F.embedding(\n    164         input, self.weight, self.padding_idx, self.max_norm,\n    165         self.norm_type, self.scale_grad_by_freq, self.sparse)\n\nFile ~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/functional.py:2237, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2231     # Note [embedding_renorm set_grad_enabled]\n   2232     # XXX: equivalent to\n   2233     # with torch.no_grad():\n   2234     #   torch.embedding_renorm_\n   2235     # remove once script supports set_grad_enabled\n   2236     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2237 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n\nIndexError: index out of range in self\n\n\n\n\ntrained_model\n\nCombinedModel(\n  (nbeats): NBEATS(\n    (trend_blocks): ModuleList(\n      (0-5): 6 x TrendBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=435, bias=True)\n          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n        )\n        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n      )\n    )\n    (seasonality_blocks): ModuleList(\n      (0-5): 6 x SeasonalityBlock(\n        (fc): ModuleList(\n          (0): Linear(in_features=40, out_features=435, bias=True)\n          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n        )\n        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n      )\n    )\n  )\n  (month_net): EmbeddingNetwork(\n    (embedding): Embedding(12, 10)\n  )\n  (day_net): EmbeddingNetwork(\n    (embedding): Embedding(31, 10)\n  )\n  (hour_net): EmbeddingNetwork(\n    (embedding): Embedding(24, 10)\n  )\n  (year_net): YearNetwork(\n    (hidden): Linear(in_features=1, out_features=25, bias=True)\n    (output): Linear(in_features=25, out_features=10, bias=True)\n  )\n  (final_layer): Sequential(\n    (0): Linear(in_features=31, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
  },
  {
    "objectID": "experiment.tahercluster.html",
    "href": "experiment.tahercluster.html",
    "title": "Taher Cluster",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nimport matplotlib.pyplot as plt\n\n\nfrom TRAIL24.data.cluster import *\n\n\n# Standardize the data\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(df_aggregations) \n\n# Apply PCA transformation\npca = PCA(n_components=1)  # Keep 95% of the variance\ndata_pca = pca.fit_transform(data_scaled)\n\n\n# Set the maximum number of clusters to test\nmax_k = 40\n\n\n# Calculate and plot elbow method\nwcss = calculate_wcss(data_pca, max_k)\nplot_elbow_method(wcss, max_k)\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(explained_variance_ratio)\nbest_k_elbow = 3\nprint(f'Best k according to elbow method: {best_k_elbow}')\n\n\n# Apply KMeans clustering\nfinal_kmeans = KMeans(n_clusters=best_k_elbow, random_state=42, n_init=90)\nfinal_kmeans.fit(data_pca)\nlabels = final_kmeans.labels_\n\n# Calculate silhouette score\nsilhouette = silhouette_score(data_pca, labels)\nprint(\"Silhouette score for the KMeans model is \", silhouette)\n\n# Calculate Davies-Bouldin score\ndbs = davies_bouldin_score(data_pca, labels)\nprint(\"Davies-Bouldin score for the KMeans model is \", dbs)",
    "crumbs": [
      "Experiments",
      "Taher Cluster"
    ]
  },
  {
    "objectID": "compression.onnx.html",
    "href": "compression.onnx.html",
    "title": "ONNX",
    "section": "",
    "text": "compression.onnx\n\nFill in a module description here\n\n\nsource\n\nscript_model\n\n script_model (model, dummy_input, path='scripted_model.pt')\n\n\nsource\n\n\nquantize_onnx\n\n quantize_onnx (model, dummy_input, onnx_path='model.onnx',\n                quant_onnx_path='model_quantized.onnx')\n\n\nfrom TRAIL24.models.nn import *\n\n\n# Example usage:\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 512\n\nnbeats_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\nmodel_cfg = {\n    'model_type': 'nbeats', \n    'model_params': nbeats_params, \n    'embedding_dim': 10, \n    'final_hidden': 256\n}\n\n\nnet = create_model(**model_cfg)\n\n\nbatch_size = 5\nnum_features = 40\n\nfeatures = torch.randn(batch_size, num_features)\nmonth = torch.randint(0, 12, (batch_size, 1))      # Random months between 1 and 12\nday = torch.randint(0, 31, (batch_size, 1))        # Random days between 1 and 31\nhour = torch.randint(0, 24, (batch_size, 1))       # Random hours between 0 and 23\n\nexample_input = features, month, day, hour\n\n\nscripted_model = script_model(net, example_input)\n\ntorch.Size([5, 40])\ntorch.Size([5, 40])\ntorch.Size([5, 40])\n\n\n\nquantize_onnx(net, example_input)\n\ntorch.Size([5, 40])\n\n\nWARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md"
  },
  {
    "objectID": "data.preprocess.html",
    "href": "data.preprocess.html",
    "title": "Preprocess",
    "section": "",
    "text": "data.preprocess\n\nFill in a module description here\n\n\nsource\n\nweekday_average\n\n weekday_average (data:dict)\n\ncompute the weekday average\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ndict\npandas dictionnary containing the data\n\n\nReturns\nlist\n\n\n\n\n\nsource\n\n\nday_segment_average\n\n day_segment_average (data:dict)\n\ncompute the daily average\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ndict\npandas dictionnary containing the data\n\n\nReturns\nlist\n\n\n\n\n\nsource\n\n\ntotal_energy_used\n\n total_energy_used (data:dict)\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ndict\npandas dictionnary containing the data\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\naverage_energy_used\n\n average_energy_used (data:dict)\n\ncompute the average energy used\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ndict\npandas dictionnary containing the data\n\n\nReturns\nlist\n\n\n\n\n\nsource\n\n\nweekend_businessday_avg\n\n weekend_businessday_avg (data:dict)\n\ncompute the weekend average\n\n\n\n\nType\nDetails\n\n\n\n\ndata\ndict\npandas dictionnary containing the data\n\n\nReturns\nlist\n\n\n\n\n\nsource\n\n\nresample_building_data\n\n resample_building_data (group:dict)\n\nresample the building data\n\n\n\n\nType\nDetails\n\n\n\n\ngroup\ndict\npandas dictionnary containing the building data\n\n\nReturns\ndict\n\n\n\n\n\nsource\n\n\noptimize_data_types\n\n optimize_data_types (df)\n\n\nsource\n\n\ncreate_lagged_features\n\n create_lagged_features (df, window_size=40, start_date='2009-07-14\n                         00:00:00')\n\n\nsource\n\n\ncreate_lagged_features\n\n create_lagged_features (df, window_size=40, start_date='2009-07-14\n                         00:00:00')\n\n\nsource\n\n\npreprocess_dataframe\n\n preprocess_dataframe (df, window_size=40, chunk_size=100)\n\n\nsource\n\n\nscale_features\n\n scale_features (train, test, columns_to_scale)\n\n\nsource\n\n\nfinalize_dataframe\n\n finalize_dataframe (df_transformed, train_start_date, train_end_date,\n                     test_start_date, test_end_date)",
    "crumbs": [
      "Data",
      "Preprocess"
    ]
  }
]