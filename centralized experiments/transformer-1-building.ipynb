{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8960734,"sourceType":"datasetVersion","datasetId":5393399}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook trains a TRANSFORMER model on one building only. dataset1 contains the data of the third building (as a random exemple) of the original dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom memory_profiler import memory_usage\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nfrom torchmetrics import MeanSquaredError\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom sklearn.preprocessing import MinMaxScaler\n%load_ext memory_profiler\n\ndef sMAPE(outputs, targets):\n    \"\"\"\n    Symmetric Mean Absolute Percentage Error (sMAPE) for evaluating the model.\n    It is the sum of the absolute difference between the predicted and actual values divided by the average of\n    the predicted and actual value, therefore giving a percentage measuring the amount of error :\n    100/n * sum(|F_t - A_t| / ((|F_t| + |A_t|) / 2)) with t = 1 to n\n\n    :param outputs: predicted values\n    :param targets: real values\n    :return: sMAPE\n    \"\"\"\n    return 100 / len(targets) * torch.sum(\n        2 * torch.abs(outputs - targets) / (torch.abs(outputs) + torch.abs(targets))\n    )\n\ndef create_sequences(data, seq_length):\n    \"\"\"\n    Function to preprocess sequential data to make it usable for training neural networks.\n    It transforms raw data into input-target pairs\n\n    :param data: the dataframe containing the data or the numpy array containing the data\n    :param seq_length: The length of the input sequences. It is the number of consecutive data points used as input to predict the next data point.\n    :return: the numpy arrays of the inputs and the targets,\n    where the inputs are sequences of consecutive data points and the targets are the immediate next data points.\n    \"\"\"\n    if len(data) < seq_length:\n        raise ValueError(\n            \"The length of the data is less than the sequence length\")\n\n    xs, ys = [], []\n    # Iterate over data indices\n    for i in range(len(data) - seq_length):\n        if type(data) is pd.DataFrame:\n            # Define inputs\n            x = data.iloc[i:i + seq_length]\n\n            # Define target\n            y = data.iloc[i + seq_length]\n\n        else:\n            # Define inputs\n            x = data[i:i + seq_length]\n\n            # Define target\n            y = data[i + seq_length]\n\n        xs.append(x)\n        ys.append(y)\n\n    # Convert lists to numpy arrays\n    xs = np.array(xs)\n    ys = np.array(ys)\n\n    # Shuffle the sequences\n    indices = np.arange(xs.shape[0])\n    np.random.shuffle(indices)\n    xs = xs[indices]\n    ys = ys[indices]\n\n    return xs, ys\n\nclass Data(Dataset):\n    \"\"\"\n    Pytorch Dataset class for the data\n    \"\"\"\n    def __init__(self, x_data, y_data):\n        self.x_data = x_data\n        self.y_data = y_data\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    def __len__(self):\n        return len(self.x_data)\n\nfile_path='/kaggle/input/dataset1/residential_1004.pkl'\nmodel_choice=\"GRU\"\ndf = pd.read_pickle(file_path)\ndf[\"ID\"] = df[\"ID\"].astype(\"category\")\ndf[\"time_code\"] = df[\"time_code\"].astype(\"uint16\")\ndf = df.set_index(\"date_time\")\n\n# Electricity consumption per hour (date with hour in the index)\ndf = df[\"consumption\"].resample(\"h\", label='right', closed='right').sum().to_frame()\ndf.head(5)\n\n# Define the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlr = 0.001\nn_epochs = 50\n\n# Scaling the input data\nsc = None  # MinMaxScaler()\nlabel_sc = None  # MinMaxScaler()\nwindow_size = 10  # number of data points used as input to predict the next data point\ndata = create_sequences(df[\"2009-07-14\":\"2010-12-15\"].values, window_size)\ndata_test = create_sequences(df[\"2010-12-15\":\"2011-01-01\"].values, window_size)\n\n# Use create_sequences to create inputs and targets\ntrain_x, train_y = create_sequences(data if sc else df[\"2009-07-14\":\"2010-12-15\"], window_size)\nprint(train_x.shape, train_y.shape)\n\ntest_x, test_y = create_sequences(data_test if label_sc else df[\"2010-12-15\":\"2011-01-01\"], window_size)\n\n# Pytorch data loaders/generators\nbatch_size = 2048\n\n# Create TensorDataset\ntrain_data = Data(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n\n# Drop the last incomplete batch\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\ndel train_x, train_y\n\n# Test data\ntest_data = Data(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=1)\ndel test_x, test_y\n\n# Positional Encoding for Transformer\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n# Model definition using Transformer\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim=1, d_model=64, nhead=8, num_layers=4, dropout=0.2):\n        super(TransformerModel, self).__init__()\n        self.encoder = nn.Linear(input_dim, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n        self.decoder = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.pos_encoder(x)\n        x = self.transformer_encoder(x)\n        x = self.decoder(x[:, -1, :])\n        return x\n\nmodel = TransformerModel().to(device)\n#model_mem_usage = memory_usage((TransformerModel,), interval=0.1)\n#print(f\"Memory usage for creating Transformer model: {max(model_mem_usage)} MiB\")\n\ndef train_function(net, criterion, optimizer, train_loader, n_epochs=5, device=torch.device(\"cuda\")):\n    for epoch in range(n_epochs):\n        for seqs, labels in train_loader:\n            # Move data to device\n            seqs, labels = seqs.float().to(device), labels.float().to(device)\n\n            # Get model outputs\n            outputs = net(seqs)\n\n            # Compute loss\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()  # Reset the graph of gradients\n            loss.backward()  # Calculate the graph of gradients\n            optimizer.step()  # Update each gradient\n        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n    return net\n\ndef test_function(net, dataloader_test, scaler, label_scaler, device=torch.device(\"cuda\")):\n    \"\"\"\n    Model evaluation on test data\n    :param net:\n    :param dataloader_test:\n    :return:\n    \"\"\"\n    # Define MSE metric\n    mse = MeanSquaredError().to(device=device)\n\n    net.eval()  # Tell the net that we are running a validation set (no dropout for example)\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():  # Do not reserve memory space for gradients\n        for seqs, labels in dataloader_test:\n            # Move data to device\n            seqs, labels = seqs.float().to(device), labels.float().to(device)\n            # Pass seqs to net and squeeze the result\n            outputs = net(seqs)\n\n            if label_scaler:\n                outputs = torch.tensor(scaler.inverse_transform(outputs), device=device)\n                labels = torch.tensor(label_scaler.inverse_transform(labels), device=device)\n\n            outputs = outputs.squeeze()\n            labels = labels.squeeze()\n\n            # Compute loss\n            mse(outputs, labels)\n            list_targets.append(labels.detach())  # Detach to remove PyTorch constraints on the values\n            list_outputs.append(outputs.detach())\n    # Compute final metric value\n    test_mse = mse.compute()\n    print(f\"Test MSE: {test_mse}\")\n\n    return torch.tensor(list_outputs, device=device), torch.tensor(list_targets, device=device), test_mse\n\n# Defining loss function and optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n%reload_ext memory_profiler\n\n# Train the model with Transformer\ntrain_mem_usage = memory_usage((train_function, (model, criterion, optimizer, train_loader, n_epochs, device)), interval=0.1)\nprint(f\"Memory usage for training function: {max(train_mem_usage)} MiB\")\n\nnet = train_function(model, criterion, optimizer, train_loader, n_epochs, device)\n\n# Evaluate the Transformer model\nnet.to(device)\nlist_outputs, list_targets, test_mse = test_function(net, test_loader, sc, label_sc, device)\n\ns_mape = round(sMAPE(list_outputs, list_targets).cpu().item(), 3)\nprint(f\"sMAPE: {s_mape}%\")\n\n# Visualizations\nplt.plot(list_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"TRANSFORMER Predictions\", markersize=3)\nplt.plot(list_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T23:08:40.358833Z","iopub.execute_input":"2024-07-17T23:08:40.359401Z","iopub.status.idle":"2024-07-17T23:11:28.578192Z","shell.execute_reply.started":"2024-07-17T23:08:40.359362Z","shell.execute_reply":"2024-07-17T23:11:28.576838Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(12469, 10, 1) (12469, 1)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 2.717456817626953\nEpoch 2, Loss: 3.0488369464874268\nEpoch 3, Loss: 2.8257153034210205\nEpoch 4, Loss: 2.8749947547912598\nEpoch 5, Loss: 2.5123119354248047\nEpoch 6, Loss: 2.2322583198547363\nEpoch 7, Loss: 2.273366928100586\nEpoch 8, Loss: 2.529101848602295\nEpoch 9, Loss: 1.965575098991394\nEpoch 10, Loss: 2.094241142272949\nEpoch 11, Loss: 2.1264524459838867\nEpoch 12, Loss: 2.0884294509887695\nEpoch 13, Loss: 1.8732662200927734\nEpoch 14, Loss: 2.072746753692627\nEpoch 15, Loss: 2.226358652114868\nEpoch 16, Loss: 2.085824966430664\nEpoch 17, Loss: 2.1527485847473145\nEpoch 18, Loss: 1.9396088123321533\nEpoch 19, Loss: 1.9086382389068604\nEpoch 20, Loss: 1.8857200145721436\nEpoch 21, Loss: 2.048654079437256\nEpoch 22, Loss: 1.877053141593933\nEpoch 23, Loss: 2.001169204711914\nEpoch 24, Loss: 1.9983865022659302\nEpoch 25, Loss: 1.834184169769287\nEpoch 26, Loss: 2.030308485031128\nEpoch 27, Loss: 2.300358772277832\nEpoch 28, Loss: 2.070951461791992\nEpoch 29, Loss: 2.1619222164154053\nEpoch 30, Loss: 1.9919536113739014\nEpoch 31, Loss: 2.0675110816955566\nEpoch 32, Loss: 1.9224481582641602\nEpoch 33, Loss: 2.097501277923584\nEpoch 34, Loss: 2.0026204586029053\nEpoch 35, Loss: 2.138723134994507\nEpoch 36, Loss: 1.8779828548431396\nEpoch 37, Loss: 1.8280901908874512\nEpoch 38, Loss: 2.13297700881958\nEpoch 39, Loss: 2.035780429840088\nEpoch 40, Loss: 2.0534048080444336\nEpoch 41, Loss: 2.051551580429077\nEpoch 42, Loss: 1.9080419540405273\nEpoch 43, Loss: 2.090376377105713\nEpoch 44, Loss: 2.1031312942504883\nEpoch 45, Loss: 1.9312713146209717\nEpoch 46, Loss: 1.8367869853973389\nEpoch 47, Loss: 1.8837594985961914\nEpoch 48, Loss: 1.9704065322875977\nEpoch 49, Loss: 2.037299156188965\nEpoch 50, Loss: 1.898625373840332\nMemory usage for training function: 969.6015625 MiB\nEpoch 1, Loss: 2.023073673248291\nEpoch 2, Loss: 1.9957773685455322\nEpoch 3, Loss: 1.9600977897644043\nEpoch 4, Loss: 2.024390935897827\nEpoch 5, Loss: 2.0834827423095703\nEpoch 6, Loss: 2.2564449310302734\nEpoch 7, Loss: 1.9628229141235352\nEpoch 8, Loss: 2.093830108642578\nEpoch 9, Loss: 2.073975086212158\nEpoch 10, Loss: 2.0560429096221924\nEpoch 11, Loss: 2.1824445724487305\nEpoch 12, Loss: 2.1116249561309814\nEpoch 13, Loss: 2.1301722526550293\nEpoch 14, Loss: 1.9253904819488525\nEpoch 15, Loss: 1.8369723558425903\nEpoch 16, Loss: 1.9226276874542236\nEpoch 17, Loss: 2.289551258087158\nEpoch 18, Loss: 2.094330310821533\nEpoch 19, Loss: 2.0212559700012207\nEpoch 20, Loss: 1.7448673248291016\nEpoch 21, Loss: 1.960558295249939\nEpoch 22, Loss: 2.114136219024658\nEpoch 23, Loss: 1.8525445461273193\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 235\u001b[0m\n\u001b[1;32m    232\u001b[0m train_mem_usage \u001b[38;5;241m=\u001b[39m memory_usage((train_function, (model, criterion, optimizer, train_loader, n_epochs, device)), interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage for training function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(train_mem_usage)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MiB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Evaluate the Transformer model\u001b[39;00m\n\u001b[1;32m    238\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[1], line 186\u001b[0m, in \u001b[0;36mtrain_function\u001b[0;34m(net, criterion, optimizer, train_loader, n_epochs, device)\u001b[0m\n\u001b[1;32m    184\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Calculate the graph of gradients\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update each gradient\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}