{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8952485,"sourceType":"datasetVersion","datasetId":5387726},{"sourceId":8962797,"sourceType":"datasetVersion","datasetId":5394756},{"sourceId":8987423,"sourceType":"datasetVersion","datasetId":5412774}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook trains NHITS model on ten building only. \"all-data\" contains the data of all the buildings in the original dataset","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchmetrics import MeanSquaredError,SymmetricMeanAbsolutePercentageError\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nfrom memory_profiler import memory_usage\nimport resource\nimport pickle\n\ndef get_memory_usage():\n    # Return current memory usage in MB\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n\nclass NHiTSBlock(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers):\n        super(NHiTSBlock, self).__init__()\n        self.hidden = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n                                    [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n        self.theta_b = nn.Linear(num_hidden, input_size)\n        self.theta_f = nn.Linear(num_hidden, output_size)\n\n    def forward(self, x):\n        for layer in self.hidden:\n            x = torch.relu(layer(x))\n        backcast = self.theta_b(x)\n        forecast = self.theta_f(x)\n        return backcast, forecast\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size, output_size, num_blocks, num_hidden, num_layers):\n        super(NHiTS, self).__init__()\n        self.blocks = nn.ModuleList([NHiTSBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks)])\n\n    def forward(self, x):\n        forecast = torch.zeros((x.size(0), self.blocks[0].theta_f.out_features), device=x.device)\n        for block in self.blocks:\n            backcast, block_forecast = block(x)\n            x = x - backcast\n            forecast = forecast + block_forecast\n        return forecast\n\n# Example usage:\ninput_size = 10  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 80\nnum_hidden = 512\nnum_layers = 20\n\n\n# Define a function to create the model for memory profiling\ndef create_model():\n    model = NHiTS(input_size, output_size, num_blocks, num_hidden, num_layers)\n    return model\n\nstart_mem = get_memory_usage()\nmodel = create_model()\nend_mem = get_memory_usage()\nprint(f\"Memory used for model creation: {end_mem - start_mem} MB\")\n# Create the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\ndef sMAPE(outputs, targets):\n    \"\"\"\n    Symmetric Mean Absolute Percentage Error (sMAPE) for evaluating the model.\n    It is the sum of the absolute difference between the predicted and actual values divided by the average of\n    the predicted and actual value, therefore giving a percentage measuring the amount of error :\n    100/n * sum(|F_t - A_t| / ((|F_t| + |A_t|) / 2)) with t = 1 to n\n\n    :param outputs: predicted values\n    :param targets: real values\n    :return: sMAPE\n    \"\"\"\n    return 100 / len(targets) * torch.sum(\n        2 * torch.abs(outputs - targets) / (torch.abs(outputs) + torch.abs(targets))\n    )\n\ndef create_sequences(data, seq_length):\n    xs, ys = [], []\n    for i in range(len(data) - seq_length):\n        x = data[i:i + seq_length]\n        y = data[i + seq_length]\n        xs.append(x)\n        ys.append(y)\n    xs = np.array(xs)\n    ys = np.array(ys)\n    return xs, ys\n\nclass Data(Dataset):\n    def __init__(self, x_data, y_data):\n        self.x_data = x_data\n        self.y_data = y_data\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    def __len__(self):\n        return len(self.x_data)\n\n# Load and preprocess data\n\nfile_path3 = '/kaggle/input/all-data/residential_all.pkl'\ndf3 = pd.read_pickle(file_path3)\nprint(\"Datra loaded\")\ndf3=df3[df3[\"ID\"] <= 1050]\ndf3[\"ID\"] = df3[\"ID\"].astype(\"category\")\ndf3[\"time_code\"] = df3[\"time_code\"].astype(\"uint16\")\ndf_test=df3[df3[\"ID\"] == 1003]\nscaler = MinMaxScaler()\ndf3 = df3.set_index([\"date_time\",\"ID\"])\ndf_test = df_test.set_index([\"date_time\",\"ID\"])\ndf3['consumption'] = scaler.fit_transform(df3[['consumption']])\ndf_test['consumption'] = scaler.transform(df_test[['consumption']])\ndf3=df3[[\"consumption\"]]\ndf_test=df_test[[\"consumption\"]]\n\ndef resample_building_data(group):\n    group = group.reset_index(level='ID')\n    # Specify columns explicitly for summing\n    resampled_group = group.resample('h').agg({'consumption': 'sum'})  # Example if 'consumption' is your numeric column\n    resampled_group['ID'] = group['ID'].iloc[0]  # Handle non-numeric separately if needed\n    resampled_group = resampled_group.set_index('ID', append=True)\n    return resampled_group\n\n\n# Group by 'building_id' and resample each group's data\ndf3 = df3.groupby('ID', group_keys=False, observed=True).apply(resample_building_data)\ndf3 = df3.sort_index()\ndf_test = df_test.groupby('ID', group_keys=False, observed=True).apply(resample_building_data)\ndf_test = df_test.sort_index()\n\ndevice = \"cuda\"\nlr = 0.001\nn_epochs = 2\nwindow_size = 10\n\ntrain_data, train_labels = create_sequences(df3[\"2009-07-14\":\"2010-12-15\"].values, window_size)\ntest_data, test_labels = create_sequences(df3[\"2010-12-15\":\"2011-01-01\"].values, window_size)\nsample_data, sample_labels = create_sequences(df_test[\"2010-12-15\":\"2011-01-01\"].values, window_size)\n\ntrain_dataset = Data(torch.FloatTensor(train_data), torch.FloatTensor(train_labels))\ntest_dataset = Data(torch.FloatTensor(test_data), torch.FloatTensor(test_labels))\nsample_dataset = Data(torch.FloatTensor(sample_data), torch.FloatTensor(sample_labels))\n\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=1024, drop_last=True)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=1024)\nsample_loader = DataLoader(sample_dataset, shuffle=False, batch_size=1)\n\ndef train_function(net, criterion, optimizer, train_loader, n_epochs=5, device=torch.device(\"cpu\")):\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, 'min',verbose =True ,threshold=0.1,patience=3,factor=0.5)\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        counter = 0\n        for seqs, labels in train_loader:\n            counter+=1\n            seqs, labels = seqs.float().to(device), labels.float().to(device)\n            seqs = seqs.view(seqs.size(0), -1)  # Ensure the input shape matches the expected shape\n            outputs = net(seqs)\n            loss = criterion(outputs, labels)\n            epoch_loss += loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if counter % 50 == 0:\n                print(f\"Batch Number {counter} {loss.item()}\")\n        scheduler.step(epoch_loss / counter) \n        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n            pickle.dump([net,optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n    return net\n\ndef test_function(net, dataloader_test, scaler, label_scaler, device=torch.device(\"cuda\"),return_data=False):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    net.eval()\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():  #to not reservate a memory space for gradients\n        for seqs, labels in dataloader_test:\n            # Move data to device\n            seqs, labels = seqs.float().to(device), labels.float().to(device)\n            # Pass seqs to net and squeeze the result\n            seqs = seqs.view(seqs.size(0), -1)\n            outputs = net(seqs)\n\n            if label_scaler:\n                outputs = torch.tensor(scaler.inverse_transform(outputs),\n                                       device=device)\n                labels = torch.tensor(label_scaler.inverse_transform(labels),\n                                      device=device)\n\n            outputs = outputs.squeeze()\n            labels = labels.squeeze()\n\n            # Compute loss\n            mse(outputs, labels)\n            smape(outputs, labels)\n            list_targets.append(labels.detach()) #detach() to remove pytorch constraints on the values\n            list_outputs.append(outputs.detach())\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n    if return_data:\n        return torch.tensor(list_outputs, device=device), torch.tensor(list_targets, device=device), test_mse\n\ncriterion = nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n#with open('/kaggle/input/model1/modelcheckpoint1.pickle', 'rb') as handle:\n#    model, optimizer = pickle.load(handle)\nprint(\"Model Started\")\n\nstart_mem = get_memory_usage()\ntime1=time.time()\nnet = train_function(model,\n                        criterion,\n                        optimizer,\n                        train_loader,\n                        n_epochs=n_epochs,\n                        device=device)\ntime2=time.time()\nprint(\"training time is \",time2-time1)\nend_mem = get_memory_usage()\nprint(f\"Memory used for model training: {end_mem - start_mem} MB\")\n\n\n\nnet.to(\"cuda\")\ntime3=time.time()\n# list_outputs, list_targets, test_mse = test_function(net, test_loader, None, None, torch.device(\"cuda\"))\ntest_function(net, test_loader, None, None, torch.device(\"cuda\"))\ntime4=time.time()\nprint(\"inference time is \",time4-time3)\n\n# s_mape = round(sMAPE(list_outputs, list_targets).cpu().item(), 3)\n# print(f\"sMAPE: {s_mape}%\")\nsample_outputs, sample_targets, sample_mse = test_function(net, sample_loader, None, None, torch.device(\"cuda\"),True)\n# Visualizations\nplt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"N-BEATS Predictions\", markersize=3)\nplt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state building number 1003\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T16:03:52.325833Z","iopub.execute_input":"2024-07-23T16:03:52.326203Z","iopub.status.idle":"2024-07-23T16:03:52.339263Z","shell.execute_reply.started":"2024-07-23T16:03:52.326173Z","shell.execute_reply":"2024-07-23T16:03:52.338349Z"},"trusted":true},"execution_count":null,"outputs":[]}]}