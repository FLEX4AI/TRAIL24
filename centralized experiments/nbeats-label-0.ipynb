{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains NBEATS model on the first cluster buildings. labeled-dataset contains the data of all the buildings in the first cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-06T00:52:31.190855Z",
     "iopub.status.busy": "2024-08-06T00:52:31.190436Z",
     "iopub.status.idle": "2024-08-06T01:11:39.555603Z",
     "shell.execute_reply": "2024-08-06T01:11:39.554379Z",
     "shell.execute_reply.started": "2024-08-06T00:52:31.190817Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Function to optimize data types\n",
    "def optimize_data_types(df):\n",
    "    # Optimize numeric columns\n",
    "    for col in df.select_dtypes(include=['int']).columns:\n",
    "        df[col] = df[col].astype('int32')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Optimize object columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique_values = len(df[col].unique())\n",
    "        num_total_values = len(df[col])\n",
    "        if num_unique_values / num_total_values < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data and remove the first and last columns\n",
    "df = pd.read_csv('/kaggle/input/labeled-dataset/df_labeled0.csv')\n",
    "df = df.iloc[:, 1:-1]\n",
    "\n",
    "# Optimize data types\n",
    "df = optimize_data_types(df)\n",
    "\n",
    "# Verify DataFrame shape\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame head:\", df.head())\n",
    "\n",
    "# Define the window size for lagged inputs\n",
    "window_size = 40  # Replace N with your desired window size\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, window_size):\n",
    "    data = []\n",
    "    for i, row in chunk.iterrows():\n",
    "        values = row.values\n",
    "        for t in range(window_size, len(values)):\n",
    "            lagged_inputs = values[t-window_size:t]\n",
    "            target_value = values[t]\n",
    "\n",
    "            timestamp = pd.Timestamp('2009-07-14 00:00:00') + pd.Timedelta(hours=t)\n",
    "            year = timestamp.year\n",
    "            month = timestamp.month\n",
    "            day = timestamp.day\n",
    "            hour = timestamp.hour\n",
    "\n",
    "            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n",
    "    return data\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "columns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\n",
    "df_transformed = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Process the data in chunks\n",
    "chunk_size = 100  # Adjust based on available memory\n",
    "for start_row in range(0, df.shape[0], chunk_size):\n",
    "    chunk = df.iloc[start_row:start_row + chunk_size]\n",
    "    data = process_chunk(chunk, window_size)\n",
    "    df_chunk_transformed = pd.DataFrame(data, columns=columns)\n",
    "    df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n",
    "\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n",
    "\n",
    "\n",
    "print(\"Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T01:11:39.559331Z",
     "iopub.status.busy": "2024-08-06T01:11:39.558847Z",
     "iopub.status.idle": "2024-08-06T01:11:55.891752Z",
     "shell.execute_reply": "2024-08-06T01:11:55.889992Z",
     "shell.execute_reply.started": "2024-08-06T01:11:39.559296Z"
    }
   },
   "outputs": [],
   "source": [
    "######CORRECT IT\n",
    "import pandas as pd\n",
    "import gc\n",
    "# Convert the date columns to datetime\n",
    "df_transformed['year1'] = df_transformed['year']\n",
    "df_transformed['month1'] = df_transformed['month']\n",
    "df_transformed['day1'] = df_transformed['day']\n",
    "df_transformed['hour1'] = df_transformed['hour']\n",
    "df_transformed[\"target1\"] = df_transformed[\"target\"]\n",
    "\n",
    "# Define the date ranges for training and testing\n",
    "train_start_date = '2009-07-14'\n",
    "train_end_date = '2010-12-15'\n",
    "test_start_date = '2010-12-15'\n",
    "test_end_date = '2011-01-01'\n",
    "\n",
    "# Convert the date columns to datetime\n",
    "df_transformed['year'] = df_transformed['year'].astype(int)\n",
    "df_transformed['month'] = df_transformed['month'].astype(int)+1\n",
    "df_transformed['day'] = df_transformed['day'].astype(int)+1\n",
    "df_transformed['hour'] = df_transformed['hour'].astype(int)+1\n",
    "\n",
    "# Create a datetime column\n",
    "df_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\n",
    "df_sample = df_transformed.loc[:12825]\n",
    "\n",
    "# Set the datetime column as the index\n",
    "df_transformed.set_index('date_time', inplace=True)\n",
    "df_sample.set_index('date_time', inplace=True)\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "df_transformed.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n",
    "df_sample.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n",
    "\n",
    "df_transformed = df_transformed.sort_index()\n",
    "df_sample = df_sample.sort_index()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = df_transformed.loc[train_start_date:train_end_date]\n",
    "\n",
    "test = df_transformed.loc[test_start_date:test_end_date]\n",
    "\n",
    "\n",
    "# Create sample data for the first building\n",
    "sample = df_sample.loc[test_start_date:test_end_date]\n",
    "\n",
    "\n",
    "\n",
    "del df_transformed, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T01:22:51.283901Z",
     "iopub.status.busy": "2024-08-06T01:22:51.283464Z",
     "iopub.status.idle": "2024-08-06T01:26:15.758549Z",
     "shell.execute_reply": "2024-08-06T01:26:15.756459Z",
     "shell.execute_reply.started": "2024-08-06T01:22:51.283866Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "import resource\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Define a function to create the model for memory profiling\n",
    "def create_model():\n",
    "    model = CombinedModel(nbits_params, embedding_dim, final_hidden)\n",
    "    return model\n",
    "\n",
    "def get_memory_usage():\n",
    "    # Return current memory usage in MB\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "class TrendBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers):\n",
    "        super(TrendBlock, self).__init__()\n",
    "        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n",
    "                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n",
    "        self.backcast_fc = nn.Linear(num_hidden, input_size)\n",
    "        self.forecast_fc = nn.Linear(num_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = torch.relu(layer(x))\n",
    "        backcast = self.backcast_fc(x)\n",
    "        forecast = self.forecast_fc(x)\n",
    "        return backcast, forecast\n",
    "\n",
    "class SeasonalityBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers):\n",
    "        super(SeasonalityBlock, self).__init__()\n",
    "        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n",
    "                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n",
    "        self.backcast_fc = nn.Linear(num_hidden, input_size)\n",
    "        self.forecast_fc = nn.Linear(num_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = torch.relu(layer(x))\n",
    "        backcast = self.backcast_fc(x)\n",
    "        forecast = self.forecast_fc(x)\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBEATS(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers, num_blocks):\n",
    "        super(NBEATS, self).__init__()\n",
    "        self.trend_blocks = nn.ModuleList([TrendBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n",
    "        self.seasonality_blocks = nn.ModuleList([SeasonalityBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        backcast, forecast = x, 0\n",
    "        for block in self.trend_blocks:\n",
    "            backcast, block_forecast = block(x)\n",
    "            x = x - backcast\n",
    "            forecast = forecast + block_forecast\n",
    "        for block in self.seasonality_blocks:\n",
    "            backcast, block_forecast = block(x)\n",
    "            x = x - backcast\n",
    "            forecast = forecast + block_forecast\n",
    "        return forecast\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class YearNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(YearNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, 25)\n",
    "        self.output = nn.Linear(25, num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        return self.output(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nbits_params, embedding_dim, final_hidden):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.nbeats = NBEATS(**nbits_params)\n",
    "        self.month_net = EmbeddingNetwork(12, embedding_dim)  # Months from 1 to 12\n",
    "        self.day_net = EmbeddingNetwork(31, embedding_dim)    # Days from 1 to 31\n",
    "        self.hour_net = EmbeddingNetwork(24, embedding_dim)   # Hours from 0 to 23\n",
    "        self.year_net = YearNetwork(1, 10)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3 + nbits_params['output_size'] + 10, final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, month, day, hour, year):\n",
    "        ts_output = self.nbeats(x)\n",
    "        month_output = self.month_net(month.long()).squeeze(1)\n",
    "        day_output = self.day_net(day.long()).squeeze(1)\n",
    "        hour_output = self.hour_net(hour.long()).squeeze(1)\n",
    "        year_output = self.year_net(year.float())\n",
    "        combined_output = torch.cat((ts_output, month_output, day_output, hour_output, year_output), dim=1)\n",
    "        final_output = self.final_layer(combined_output)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_size = 40  # Length of input time series\n",
    "output_size = 1  # Length of output time series (forecast)\n",
    "num_blocks = 12\n",
    "num_hidden = 512\n",
    "num_layers = 8\n",
    "embedding_dim = 10\n",
    "final_hidden = 256\n",
    "\n",
    "nbits_params = {\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'num_blocks': num_blocks,\n",
    "    'num_hidden': num_hidden,\n",
    "    'num_layers': num_layers\n",
    "}\n",
    "\n",
    "start_mem = get_memory_usage()\n",
    "net = create_model()\n",
    "end_mem = get_memory_usage()\n",
    "print(f\"Memory used for model creation: {end_mem - start_mem} MB\")\n",
    "\n",
    "device = \"cuda\"\n",
    "net.to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx].astype(\"float32\")\n",
    "        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n",
    "        year = torch.FloatTensor([row[-5]]).to(torch.int)\n",
    "        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is already 0-indexed by OrdinalEncoder\n",
    "        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is already 0-indexed by OrdinalEncoder\n",
    "        hour = torch.FloatTensor([row[-2]]).to(torch.int)   # Hour is already 0-indexed by OrdinalEncoder\n",
    "        label = torch.FloatTensor([row[-1]])  # The last column\n",
    "        return features, year, month, day, hour, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "def create_data_loaders(data, chunk_size, batch_size):\n",
    "    chunks = np.array_split(data, len(data) // chunk_size)\n",
    "    data_loaders = []\n",
    "    for chunk in chunks:\n",
    "        dataset = CustomDataset(chunk)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        data_loaders.append(data_loader)\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "lr = 0.0005\n",
    "n_epochs = 10\n",
    "window_size = 40\n",
    "chunk_size = 122880\n",
    "batch_size = 1024*12*5\n",
    "\n",
    "# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\n",
    "train_loaders = create_data_loaders(train, chunk_size, batch_size)\n",
    "test_loaders = create_data_loaders(test, chunk_size, batch_size)\n",
    "sample_loaders = create_data_loaders(sample, 100, 1)\n",
    "print(\"data loaders are ready\")\n",
    "\n",
    "del train, test, sample\n",
    "gc.collect()\n",
    "\n",
    "def train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cpu\")):\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        counter = 0\n",
    "        for data_loader in data_loaders:\n",
    "            for features, year, month, day, hour, labels in data_loader:\n",
    "                features = features.to(device)\n",
    "                year = year.to(device)\n",
    "                month = month.to(device)\n",
    "                day = day.to(device)\n",
    "                hour = hour.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(features, month, day, hour, year)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(\"c bon\")\n",
    "        scheduler.step(epoch_loss / len(data_loaders))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n",
    "        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n",
    "            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        collected = gc.collect()\n",
    "    return net\n",
    "\n",
    "def test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n",
    "    mse = MeanSquaredError().to(device)\n",
    "    smape = SymmetricMeanAbsolutePercentageError().to(device)\n",
    "    net.eval()\n",
    "    list_outputs = []\n",
    "    list_targets = []\n",
    "    with torch.no_grad():  # to not reserve a memory space for gradients\n",
    "        for data_loader in data_loaders:\n",
    "            for features, year, month, day, hour, labels in data_loader:\n",
    "                features = features.to(device)\n",
    "                year = year.to(device)\n",
    "                month = month.to(device)\n",
    "                day = day.to(device)\n",
    "                hour = hour.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(features, month, day, hour, year)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                mse(outputs, labels)\n",
    "                smape(outputs, labels)\n",
    "                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n",
    "                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n",
    "    test_mse = mse.compute()\n",
    "    test_smape = smape.compute()\n",
    "    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n",
    "    if return_data:\n",
    "        return torch.cat(list_outputs), torch.cat(list_targets), test_mse\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "print(\"Model Started\")\n",
    "\n",
    "start_mem = get_memory_usage()\n",
    "time1 = time.time()\n",
    "net = train_function(net, criterion, optimizer, train_loaders, n_epochs=n_epochs, device=torch.device(device))\n",
    "time2 = time.time()\n",
    "print(\"training time is \", time2 - time1)\n",
    "end_mem = get_memory_usage()\n",
    "print(f\"Memory used for model training: {end_mem - start_mem} MB\")\n",
    "\n",
    "net.to(device)\n",
    "time3 = time.time()\n",
    "test_function(net, test_loaders, None, None, torch.device(device))\n",
    "time4 = time.time()\n",
    "print(\"inference time is \", time4 - time3)\n",
    "\n",
    "sample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(device), True)\n",
    "plt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NBEATS Predictions\", markersize=3)\n",
    "plt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for Electricity state 1st building\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5480827,
     "sourceId": 9083908,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5505677,
     "sourceId": 9120514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5510805,
     "sourceId": 9127732,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
