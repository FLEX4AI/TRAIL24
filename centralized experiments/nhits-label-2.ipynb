{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9159735,"sourceType":"datasetVersion","datasetId":5533795}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook trains NHITS model on the third cluster buildings. big-label-2 contains the data of all the buildings in the third cluster","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Function to optimize data types\ndef optimize_data_types(df):\n    # Optimize numeric columns\n    for col in df.select_dtypes(include=['int']).columns:\n        df[col] = df[col].astype('int32')\n    \n    for col in df.select_dtypes(include=['float']).columns:\n        df[col] = df[col].astype('float32')\n\n    # Optimize object columns\n    for col in df.select_dtypes(include=['object']).columns:\n        num_unique_values = len(df[col].unique())\n        num_total_values = len(df[col])\n        if num_unique_values / num_total_values < 0.5:\n            df[col] = df[col].astype('category')\n    \n    return df\n\n# Load the data and remove the first and last columns\ndf = pd.read_csv('/kaggle/input/big-label-2/df_labeled2.csv')\ndf = df.iloc[:, 1:-1]\n\n# Optimize data types\ndf = optimize_data_types(df)\n\n# Verify DataFrame shape\nprint(\"DataFrame shape:\", df.shape)\nprint(\"DataFrame head:\", df.head())\n\n# Define the window size for lagged inputs\nwindow_size = 40  # Replace N with your desired window size\n\n# Function to process each chunk\ndef process_chunk(chunk, window_size):\n    data = []\n    for i, row in chunk.iterrows():\n        values = row.values\n        for t in range(window_size, len(values)):\n            lagged_inputs = values[t-window_size:t]\n            target_value = values[t]\n\n            timestamp = pd.Timestamp('2009-07-14 00:00:00') + pd.Timedelta(hours=t)\n            year = timestamp.year\n            month = timestamp.month\n            day = timestamp.day\n            hour = timestamp.hour\n\n            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n    return data\n\n# Create an empty DataFrame to store results\ncolumns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\ndf_transformed = pd.DataFrame(columns=columns)\n\n# Process the data in chunks\nchunk_size = 100  # Adjust based on available memory\nfor start_row in range(0, df.shape[0], chunk_size):\n    chunk = df.iloc[start_row:start_row + chunk_size]\n    data = process_chunk(chunk, window_size)\n    df_chunk_transformed = pd.DataFrame(data, columns=columns)\n    df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n\n\nordinal_encoder = OrdinalEncoder()\ndf_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n\n\nprint(\"Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######CORRECT IT\nimport pandas as pd\nimport gc\n# Convert the date columns to datetime\ndf_transformed['year1'] = df_transformed['year']\ndf_transformed['month1'] = df_transformed['month']\ndf_transformed['day1'] = df_transformed['day']\ndf_transformed['hour1'] = df_transformed['hour']\ndf_transformed[\"target1\"] = df_transformed[\"target\"]\n\n# Define the date ranges for training and testing\ntrain_start_date = '2009-07-14'\ntrain_end_date = '2010-12-15'\ntest_start_date = '2010-12-15'\ntest_end_date = '2011-01-01'\n\n# Convert the date columns to datetime\ndf_transformed['year'] = df_transformed['year'].astype(int)\ndf_transformed['month'] = df_transformed['month'].astype(int)+1\ndf_transformed['day'] = df_transformed['day'].astype(int)+1\ndf_transformed['hour'] = df_transformed['hour'].astype(int)+1\n\n# Create a datetime column\ndf_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\ndf_sample = df_transformed.loc[:12825]\n\n# Set the datetime column as the index\ndf_transformed.set_index('date_time', inplace=True)\ndf_sample.set_index('date_time', inplace=True)\n\n# Drop the irrelevant columns\ndf_transformed.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\ndf_sample.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n\ndf_transformed = df_transformed.sort_index()\ndf_sample = df_sample.sort_index()\n\n# Split the data into training and testing sets\ntrain = df_transformed.loc[train_start_date:train_end_date]\n\ntest = df_transformed.loc[test_start_date:test_end_date]\n\n\n# Create sample data for the first building\nsample = df_sample.loc[test_start_date:test_end_date]\n\n\n\ndel df_transformed, df\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\nfrom torch.utils.data import DataLoader, Dataset\nimport time\nfrom memory_profiler import memory_usage\nimport resource\nimport pickle\nimport gc\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Define a function to create the model for memory profiling\ndef create_model():\n    model = CombinedModel(nhits_params, embedding_dim, final_hidden)\n    return model\n\ndef get_memory_usage():\n    # Return current memory usage in MB\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n\nclass NHiTSBlock(nn.Module):\n    def __init__(self, input_size, output_size, num_hidden, num_layers):\n        super(NHiTSBlock, self).__init__()\n        self.hidden = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n                                    [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n        self.theta_b = nn.Linear(num_hidden, input_size)\n        self.theta_f = nn.Linear(num_hidden, output_size)\n\n    def forward(self, x):\n        for layer in self.hidden:\n            x = torch.relu(layer(x))\n        backcast = self.theta_b(x)\n        forecast = self.theta_f(x)\n        return backcast, forecast\n\nclass NHiTS(nn.Module):\n    def __init__(self, input_size, output_size, num_blocks, num_hidden, num_layers):\n        super(NHiTS, self).__init__()\n        self.blocks = nn.ModuleList([NHiTSBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks)])\n\n    def forward(self, x):\n        forecast = torch.zeros((x.size(0), self.blocks[0].theta_f.out_features), device=x.device)\n        for block in self.blocks:\n            backcast, block_forecast = block(x)\n            x = x - backcast\n            forecast = forecast + block_forecast\n        return forecast\n\nclass EmbeddingNetwork(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super(EmbeddingNetwork, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n\n    def forward(self, x):\n        return self.embedding(x)\n\nclass YearNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super(YearNetwork, self).__init__()\n        self.hidden = nn.Linear(input_dim, 25)\n        self.output = nn.Linear(25, num_hidden)\n\n    def forward(self, x):\n        x = torch.relu(self.hidden(x))\n        return self.output(x)\n\nclass CombinedModel(nn.Module):\n    def __init__(self, nhits_params, embedding_dim, final_hidden):\n        super(CombinedModel, self).__init__()\n        self.nhits = NHiTS(**nhits_params)\n        self.month_net = EmbeddingNetwork(12, embedding_dim)  # Months from 1 to 12\n        self.day_net = EmbeddingNetwork(31, embedding_dim)    # Days from 1 to 31\n        self.hour_net = EmbeddingNetwork(24, embedding_dim)   # Hours from 0 to 23\n        self.year_net = YearNetwork(1, 10)\n        self.final_layer = nn.Sequential(\n            nn.Linear(embedding_dim * 3 + nhits_params['output_size'] + 10, final_hidden),\n            nn.ReLU(),\n            nn.Linear(final_hidden, 1)\n        )\n\n    def forward(self, x, month, day, hour, year):\n        ts_output = self.nhits(x)\n        month_output = self.month_net(month.long()).squeeze(1)\n        day_output = self.day_net(day.long()).squeeze(1)\n        hour_output = self.hour_net(hour.long()).squeeze(1)\n        year_output = self.year_net(year.float())\n        combined_output = torch.cat((ts_output, month_output, day_output, hour_output, year_output), dim=1)\n        final_output = self.final_layer(combined_output)\n        return final_output\n\n\n# Example usage:\ninput_size = 40  # Length of input time series\noutput_size = 1  # Length of output time series (forecast)\nnum_blocks = 12\nnum_hidden = 512\nnum_layers = 8\nembedding_dim = 10\nfinal_hidden = 256\n\nnhits_params = {\n    'input_size': input_size,\n    'output_size': output_size,\n    'num_blocks': num_blocks,\n    'num_hidden': num_hidden,\n    'num_layers': num_layers\n}\n\nstart_mem = get_memory_usage()\nnet = create_model()\nend_mem = get_memory_usage()\nprint(f\"Memory used for model creation: {end_mem - start_mem} MB\")\n\ndevice = \"cuda\"\nnet.to(device)\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx].astype(\"float32\")\n        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n        year = torch.FloatTensor([row[-5]]).to(torch.int)\n        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is already 0-indexed by OrdinalEncoder\n        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is already 0-indexed by OrdinalEncoder\n        hour = torch.FloatTensor([row[-2]]).to(torch.int)   # Hour is already 0-indexed by OrdinalEncoder\n        label = torch.FloatTensor([row[-1]])  # The last column\n        return features, year, month, day, hour, label\n\n    def __len__(self):\n        return len(self.dataframe)\n\ndef create_data_loaders(data, chunk_size, batch_size):\n    chunks = np.array_split(data, len(data) // chunk_size)\n    data_loaders = []\n    for chunk in chunks:\n        dataset = CustomDataset(chunk)\n        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        data_loaders.append(data_loader)\n    return data_loaders\n\n\nlr = 0.0005\nn_epochs = 10\nwindow_size = 40\nchunk_size = 120000\nbatch_size = 1024*12*5\n\n# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\ntrain_loaders = create_data_loaders(train, chunk_size, batch_size)\ntest_loaders = create_data_loaders(test, chunk_size, batch_size)\nsample_loaders = create_data_loaders(sample, 100, 1)\nprint(\"data loaders are ready\")\n\ndel train, test, sample\ngc.collect()\n\ndef train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cpu\")):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    from torch.optim.lr_scheduler import ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        counter = 0\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in data_loader:\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                loss = criterion(outputs, labels)\n                epoch_loss += loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                outputs = outputs.squeeze()\n                labels = labels.squeeze()\n\n                mse(outputs, labels)\n                smape(outputs, labels)\n                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n        scheduler.step(epoch_loss / len(data_loaders))\n        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n        collected = gc.collect()\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"train MSE: {test_mse} , SMAPE {test_smape}\")\n    return net\n\ndef test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n    mse = MeanSquaredError().to(device)\n    smape = SymmetricMeanAbsolutePercentageError().to(device)\n    net.eval()\n    list_outputs = []\n    list_targets = []\n    with torch.no_grad():  # to not reserve a memory space for gradients\n        for data_loader in data_loaders:\n            for features, year, month, day, hour, labels in data_loader:\n                features = features.to(device)\n                year = year.to(device)\n                month = month.to(device)\n                day = day.to(device)\n                hour = hour.to(device)\n                labels = labels.to(device)\n\n                outputs = net(features, month, day, hour, year)\n                outputs = outputs.squeeze()\n                labels = labels.squeeze()\n\n                mse(outputs, labels)\n                smape(outputs, labels)\n                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n    test_mse = mse.compute()\n    test_smape = smape.compute()\n    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n    if return_data:\n        return torch.cat(list_outputs), torch.cat(list_targets), test_mse\n\ncriterion = nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n\nprint(\"Model Started\")\n\nstart_mem = get_memory_usage()\ntime1 = time.time()\nnet = train_function(net, criterion, optimizer, train_loaders, n_epochs=n_epochs, device=torch.device(\"cuda\"))\ntime2 = time.time()\nprint(\"training time is \", time2 - time1)\nend_mem = get_memory_usage()\nprint(f\"Memory used for model training: {end_mem - start_mem} MB\")\n\nnet.to(\"cuda\")\ntime3 = time.time()\ntest_function(net, test_loaders, None, None, torch.device(\"cuda\"))\ntime4 = time.time()\nprint(\"inference time is \", time4 - time3)\n\nsample_outputs, sample_targets, sample_mse = test_function(net, sample_loaders, None, None, torch.device(\"cuda\"), True)\nplt.plot(sample_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"NHiTS Predictions\", markersize=3)\nplt.plot(sample_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\nplt.ylabel(\"Energy Consumption (MW)\")\nplt.title(f\"Energy Consumption for Electricity state 1st building\")\nplt.legend()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}