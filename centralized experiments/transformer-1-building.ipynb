{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains a TRANSFORMER model on one building only. dataset1 contains the data of the third building (as a random exemple) of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T23:08:40.359401Z",
     "iopub.status.busy": "2024-07-17T23:08:40.358833Z",
     "iopub.status.idle": "2024-07-17T23:11:28.578192Z",
     "shell.execute_reply": "2024-07-17T23:11:28.576838Z",
     "shell.execute_reply.started": "2024-07-17T23:08:40.359362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12469, 10, 1) (12469, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.717456817626953\n",
      "Epoch 2, Loss: 3.0488369464874268\n",
      "Epoch 3, Loss: 2.8257153034210205\n",
      "Epoch 4, Loss: 2.8749947547912598\n",
      "Epoch 5, Loss: 2.5123119354248047\n",
      "Epoch 6, Loss: 2.2322583198547363\n",
      "Epoch 7, Loss: 2.273366928100586\n",
      "Epoch 8, Loss: 2.529101848602295\n",
      "Epoch 9, Loss: 1.965575098991394\n",
      "Epoch 10, Loss: 2.094241142272949\n",
      "Epoch 11, Loss: 2.1264524459838867\n",
      "Epoch 12, Loss: 2.0884294509887695\n",
      "Epoch 13, Loss: 1.8732662200927734\n",
      "Epoch 14, Loss: 2.072746753692627\n",
      "Epoch 15, Loss: 2.226358652114868\n",
      "Epoch 16, Loss: 2.085824966430664\n",
      "Epoch 17, Loss: 2.1527485847473145\n",
      "Epoch 18, Loss: 1.9396088123321533\n",
      "Epoch 19, Loss: 1.9086382389068604\n",
      "Epoch 20, Loss: 1.8857200145721436\n",
      "Epoch 21, Loss: 2.048654079437256\n",
      "Epoch 22, Loss: 1.877053141593933\n",
      "Epoch 23, Loss: 2.001169204711914\n",
      "Epoch 24, Loss: 1.9983865022659302\n",
      "Epoch 25, Loss: 1.834184169769287\n",
      "Epoch 26, Loss: 2.030308485031128\n",
      "Epoch 27, Loss: 2.300358772277832\n",
      "Epoch 28, Loss: 2.070951461791992\n",
      "Epoch 29, Loss: 2.1619222164154053\n",
      "Epoch 30, Loss: 1.9919536113739014\n",
      "Epoch 31, Loss: 2.0675110816955566\n",
      "Epoch 32, Loss: 1.9224481582641602\n",
      "Epoch 33, Loss: 2.097501277923584\n",
      "Epoch 34, Loss: 2.0026204586029053\n",
      "Epoch 35, Loss: 2.138723134994507\n",
      "Epoch 36, Loss: 1.8779828548431396\n",
      "Epoch 37, Loss: 1.8280901908874512\n",
      "Epoch 38, Loss: 2.13297700881958\n",
      "Epoch 39, Loss: 2.035780429840088\n",
      "Epoch 40, Loss: 2.0534048080444336\n",
      "Epoch 41, Loss: 2.051551580429077\n",
      "Epoch 42, Loss: 1.9080419540405273\n",
      "Epoch 43, Loss: 2.090376377105713\n",
      "Epoch 44, Loss: 2.1031312942504883\n",
      "Epoch 45, Loss: 1.9312713146209717\n",
      "Epoch 46, Loss: 1.8367869853973389\n",
      "Epoch 47, Loss: 1.8837594985961914\n",
      "Epoch 48, Loss: 1.9704065322875977\n",
      "Epoch 49, Loss: 2.037299156188965\n",
      "Epoch 50, Loss: 1.898625373840332\n",
      "Memory usage for training function: 969.6015625 MiB\n",
      "Epoch 1, Loss: 2.023073673248291\n",
      "Epoch 2, Loss: 1.9957773685455322\n",
      "Epoch 3, Loss: 1.9600977897644043\n",
      "Epoch 4, Loss: 2.024390935897827\n",
      "Epoch 5, Loss: 2.0834827423095703\n",
      "Epoch 6, Loss: 2.2564449310302734\n",
      "Epoch 7, Loss: 1.9628229141235352\n",
      "Epoch 8, Loss: 2.093830108642578\n",
      "Epoch 9, Loss: 2.073975086212158\n",
      "Epoch 10, Loss: 2.0560429096221924\n",
      "Epoch 11, Loss: 2.1824445724487305\n",
      "Epoch 12, Loss: 2.1116249561309814\n",
      "Epoch 13, Loss: 2.1301722526550293\n",
      "Epoch 14, Loss: 1.9253904819488525\n",
      "Epoch 15, Loss: 1.8369723558425903\n",
      "Epoch 16, Loss: 1.9226276874542236\n",
      "Epoch 17, Loss: 2.289551258087158\n",
      "Epoch 18, Loss: 2.094330310821533\n",
      "Epoch 19, Loss: 2.0212559700012207\n",
      "Epoch 20, Loss: 1.7448673248291016\n",
      "Epoch 21, Loss: 1.960558295249939\n",
      "Epoch 22, Loss: 2.114136219024658\n",
      "Epoch 23, Loss: 1.8525445461273193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m\n\u001b[1;32m    232\u001b[0m train_mem_usage \u001b[38;5;241m=\u001b[39m memory_usage((train_function, (model, criterion, optimizer, train_loader, n_epochs, device)), interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage for training function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(train_mem_usage)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MiB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Evaluate the Transformer model\u001b[39;00m\n\u001b[1;32m    238\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[1], line 186\u001b[0m, in \u001b[0;36mtrain_function\u001b[0;34m(net, criterion, optimizer, train_loader, n_epochs, device)\u001b[0m\n\u001b[1;32m    184\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Calculate the graph of gradients\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update each gradient\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "def sMAPE(outputs, targets):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (sMAPE) for evaluating the model.\n",
    "    It is the sum of the absolute difference between the predicted and actual values divided by the average of\n",
    "    the predicted and actual value, therefore giving a percentage measuring the amount of error :\n",
    "    100/n * sum(|F_t - A_t| / ((|F_t| + |A_t|) / 2)) with t = 1 to n\n",
    "\n",
    "    :param outputs: predicted values\n",
    "    :param targets: real values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    return 100 / len(targets) * torch.sum(\n",
    "        2 * torch.abs(outputs - targets) / (torch.abs(outputs) + torch.abs(targets))\n",
    "    )\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Function to preprocess sequential data to make it usable for training neural networks.\n",
    "    It transforms raw data into input-target pairs\n",
    "\n",
    "    :param data: the dataframe containing the data or the numpy array containing the data\n",
    "    :param seq_length: The length of the input sequences. It is the number of consecutive data points used as input to predict the next data point.\n",
    "    :return: the numpy arrays of the inputs and the targets,\n",
    "    where the inputs are sequences of consecutive data points and the targets are the immediate next data points.\n",
    "    \"\"\"\n",
    "    if len(data) < seq_length:\n",
    "        raise ValueError(\n",
    "            \"The length of the data is less than the sequence length\")\n",
    "\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(data) - seq_length):\n",
    "        if type(data) is pd.DataFrame:\n",
    "            # Define inputs\n",
    "            x = data.iloc[i:i + seq_length]\n",
    "\n",
    "            # Define target\n",
    "            y = data.iloc[i + seq_length]\n",
    "\n",
    "        else:\n",
    "            # Define inputs\n",
    "            x = data[i:i + seq_length]\n",
    "\n",
    "            # Define target\n",
    "            y = data[i + seq_length]\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Shuffle the sequences\n",
    "    indices = np.arange(xs.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    xs = xs[indices]\n",
    "    ys = ys[indices]\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "class Data(Dataset):\n",
    "    \"\"\"\n",
    "    Pytorch Dataset class for the data\n",
    "    \"\"\"\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "file_path='/kaggle/input/dataset1/residential_1004.pkl'\n",
    "model_choice=\"GRU\"\n",
    "df = pd.read_pickle(file_path)\n",
    "df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "df[\"time_code\"] = df[\"time_code\"].astype(\"uint16\")\n",
    "df = df.set_index(\"date_time\")\n",
    "\n",
    "# Electricity consumption per hour (date with hour in the index)\n",
    "df = df[\"consumption\"].resample(\"h\", label='right', closed='right').sum().to_frame()\n",
    "df.head(5)\n",
    "\n",
    "# Define the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 0.001\n",
    "n_epochs = 50\n",
    "\n",
    "# Scaling the input data\n",
    "sc = None  # MinMaxScaler()\n",
    "label_sc = None  # MinMaxScaler()\n",
    "window_size = 10  # number of data points used as input to predict the next data point\n",
    "data = create_sequences(df[\"2009-07-14\":\"2010-12-15\"].values, window_size)\n",
    "data_test = create_sequences(df[\"2010-12-15\":\"2011-01-01\"].values, window_size)\n",
    "\n",
    "# Use create_sequences to create inputs and targets\n",
    "train_x, train_y = create_sequences(data if sc else df[\"2009-07-14\":\"2010-12-15\"], window_size)\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "test_x, test_y = create_sequences(data_test if label_sc else df[\"2010-12-15\":\"2011-01-01\"], window_size)\n",
    "\n",
    "# Pytorch data loaders/generators\n",
    "batch_size = 2048\n",
    "\n",
    "# Create TensorDataset\n",
    "train_data = Data(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "\n",
    "# Drop the last incomplete batch\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "del train_x, train_y\n",
    "\n",
    "# Test data\n",
    "test_data = Data(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=1)\n",
    "del test_x, test_y\n",
    "\n",
    "# Positional Encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Model definition using Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=8, num_layers=4, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "model = TransformerModel().to(device)\n",
    "#model_mem_usage = memory_usage((TransformerModel,), interval=0.1)\n",
    "#print(f\"Memory usage for creating Transformer model: {max(model_mem_usage)} MiB\")\n",
    "\n",
    "def train_function(net, criterion, optimizer, train_loader, n_epochs=5, device=torch.device(\"cuda\")):\n",
    "    for epoch in range(n_epochs):\n",
    "        for seqs, labels in train_loader:\n",
    "            # Move data to device\n",
    "            seqs, labels = seqs.float().to(device), labels.float().to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = net(seqs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()  # Reset the graph of gradients\n",
    "            loss.backward()  # Calculate the graph of gradients\n",
    "            optimizer.step()  # Update each gradient\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    return net\n",
    "\n",
    "def test_function(net, dataloader_test, scaler, label_scaler, device=torch.device(\"cuda\")):\n",
    "    \"\"\"\n",
    "    Model evaluation on test data\n",
    "    :param net:\n",
    "    :param dataloader_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Define MSE metric\n",
    "    mse = MeanSquaredError().to(device=device)\n",
    "\n",
    "    net.eval()  # Tell the net that we are running a validation set (no dropout for example)\n",
    "    list_outputs = []\n",
    "    list_targets = []\n",
    "    with torch.no_grad():  # Do not reserve memory space for gradients\n",
    "        for seqs, labels in dataloader_test:\n",
    "            # Move data to device\n",
    "            seqs, labels = seqs.float().to(device), labels.float().to(device)\n",
    "            # Pass seqs to net and squeeze the result\n",
    "            outputs = net(seqs)\n",
    "\n",
    "            if label_scaler:\n",
    "                outputs = torch.tensor(scaler.inverse_transform(outputs), device=device)\n",
    "                labels = torch.tensor(label_scaler.inverse_transform(labels), device=device)\n",
    "\n",
    "            outputs = outputs.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            mse(outputs, labels)\n",
    "            list_targets.append(labels.detach())  # Detach to remove PyTorch constraints on the values\n",
    "            list_outputs.append(outputs.detach())\n",
    "    # Compute final metric value\n",
    "    test_mse = mse.compute()\n",
    "    print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "    return torch.tensor(list_outputs, device=device), torch.tensor(list_targets, device=device), test_mse\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "%reload_ext memory_profiler\n",
    "\n",
    "# Train the model with Transformer\n",
    "train_mem_usage = memory_usage((train_function, (model, criterion, optimizer, train_loader, n_epochs, device)), interval=0.1)\n",
    "print(f\"Memory usage for training function: {max(train_mem_usage)} MiB\")\n",
    "\n",
    "net = train_function(model, criterion, optimizer, train_loader, n_epochs, device)\n",
    "\n",
    "# Evaluate the Transformer model\n",
    "net.to(device)\n",
    "list_outputs, list_targets, test_mse = test_function(net, test_loader, sc, label_sc, device)\n",
    "\n",
    "s_mape = round(sMAPE(list_outputs, list_targets).cpu().item(), 3)\n",
    "print(f\"sMAPE: {s_mape}%\")\n",
    "\n",
    "# Visualizations\n",
    "plt.plot(list_outputs.to(\"cpu\"), \"-o\", color=\"blue\", label=\"TRANSFORMER Predictions\", markersize=3)\n",
    "plt.plot(list_targets.to(\"cpu\"), color=\"red\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for Electricity state\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5393399,
     "sourceId": 8960734,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
