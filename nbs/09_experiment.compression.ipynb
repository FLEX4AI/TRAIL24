{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8bd2a14d-f3e2-449e-b518-64138846c2a8",
   "metadata": {},
   "source": [
    "---\n",
    "description: Compression Notebook\n",
    "output-file: experiment.compression.html\n",
    "title: Compression\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c2079-7709-4d41-a538-70fa86a489d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from TRAIL24.compression.pruning import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#from torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "#from memory_profiler import memory_usage\n",
    "import resource\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchmetrics import MeanSquaredError, SymmetricMeanAbsolutePercentageError\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb77284-c0c4-4fbc-a35f-6a2a671242fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKKKK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8bcd617f36442eaeed4be527207c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ea773372a74b70943e1d6a1110ae33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdf3ca5d35641708a59ef8d4a3bf563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef61cd0f51a94011a8b32f16fcdfb49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab94c4a38534922a6e9c87760a2a61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8eaec55f454b64a4ad8904f534c923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f5d139133d4cbeb7b4355dd0be9956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754523b6bfc247049faa31e08c188e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5fce1b28b44f548e7c201303776f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion successful.\n",
      "Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\n",
      "year       int64\n",
      "month    float64\n",
      "day      float64\n",
      "hour     float64\n",
      "dtype: object\n",
      "[ 6.  7.  8.  9. 10. 11.  0.  1.  2.  3.  4.  5.]\n",
      "[14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30.  0.\n",
      "  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "[16. 17. 18. 19. 20. 21. 22. 23.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9.\n",
      " 10. 11. 12. 13. 14. 15.]\n"
     ]
    },
    {
     "ename": "SettingWithCopyError",
     "evalue": "\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSettingWithCopyError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2151806/3735520156.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/df_cluster_0.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5577\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m250.0\u001b[0m   \u001b[0;36m150.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5578\u001b[0m         \u001b[0mfalcon\u001b[0m  \u001b[0mspeed\u001b[0m   \u001b[0;36m320.0\u001b[0m   \u001b[0;36m250.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5584\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4788\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4792\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4794\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, result, verify_is_copy)\u001b[0m\n\u001b[1;32m   4895\u001b[0m         \u001b[0;31m# decision that we may revisit in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4896\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4897\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4898\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4899\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, clear, verify_is_copy, inplace)\u001b[0m\n\u001b[1;32m   4012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify_is_copy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"referent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, t, force)\u001b[0m\n\u001b[1;32m   4469\u001b[0m                 \u001b[0;34m\"indexing.html#returning-a-view-versus-a-copy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4470\u001b[0m             )\n\u001b[1;32m   4471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mSettingWithCopyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"warn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSettingWithCopyWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSettingWithCopyError\u001b[0m: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../data/df_cluster_0.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "print('OKKKK')\n",
    "\n",
    "num_rows = len(df)\n",
    "\n",
    "# Calculate the halfway point\n",
    "halfway_point = num_rows // 4\n",
    "\n",
    "# Select the first half of the rows\n",
    "df = df.iloc[:halfway_point]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Function to optimize data types\n",
    "def optimize_data_types(df):\n",
    "    # Optimize numeric columns\n",
    "    for col in df.select_dtypes(include=['int']).columns:\n",
    "        df[col] = df[col].astype('int32')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Optimize object columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique_values = len(df[col].unique())\n",
    "        num_total_values = len(df[col])\n",
    "        if num_unique_values / num_total_values < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data and remove the first and last columns\n",
    "#df = pd.read_csv('df_labeled0.csv')\n",
    "df = df.iloc[:, 1:-1]\n",
    "\n",
    "# Optimize data types\n",
    "df = optimize_data_types(df)\n",
    "\n",
    "# Define the window size for lagged inputs\n",
    "window_size = 40  # Replace N with your desired window size\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, window_size):\n",
    "    data = []\n",
    "    for i, row in tqdm(chunk.iterrows()):\n",
    "        values = row.values\n",
    "        for t in range(window_size, len(values)):\n",
    "            lagged_inputs = values[t-window_size:t]\n",
    "            target_value = values[t]\n",
    "\n",
    "            # Calculate realistic datetime values using the index\n",
    "            base_date = pd.Timestamp('2009-07-14 00:00:00')\n",
    "            timestamp = base_date + pd.Timedelta(hours=t)\n",
    "            year = timestamp.year\n",
    "            month = timestamp.month\n",
    "            day = timestamp.day\n",
    "            hour = timestamp.hour\n",
    "\n",
    "            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n",
    "    return data\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "columns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\n",
    "df_transformed = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Process the data in chunks\n",
    "chunk_size = 100  # Adjust based on available memory\n",
    "for start_row in tqdm(range(0, df.shape[0], chunk_size)):\n",
    "    chunk = df.iloc[start_row:start_row + chunk_size]\n",
    "    data = process_chunk(chunk, window_size)\n",
    "    df_chunk_transformed = pd.DataFrame(data, columns=columns)\n",
    "    df_transformed = pd.concat([df_transformed, df_chunk_transformed], ignore_index=True)\n",
    "\n",
    "# Make sure year, month, day, and hour are integers\n",
    "df_transformed['year'] = df_transformed['year'].astype(int)\n",
    "df_transformed['month'] = df_transformed['month'].astype(int)\n",
    "df_transformed['day'] = df_transformed['day'].astype(int)\n",
    "df_transformed['hour'] = df_transformed['hour'].astype(int)\n",
    "\n",
    "# Create datetime column\n",
    "try:\n",
    "    df_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\n",
    "    print(\"Date conversion successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during date conversion:\", e)\n",
    "\n",
    "# Apply OrdinalEncoder to month, day, hour columns\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n",
    "\n",
    "print(\"Transformation complete. The new dataset is saved as 'transformed_dataset.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_transformed[['year', 'month', 'day', 'hour']].dtypes)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "print(df_transformed['month'].unique())  # Should be within 1-12\n",
    "print(df_transformed['day'].unique())    # Should be within 1-31\n",
    "print(df_transformed['hour'].unique())   # Should be within 0-23\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Convert the date columns to datetime\n",
    "df_transformed['year1'] = df_transformed['year']\n",
    "df_transformed['month1'] = df_transformed['month']\n",
    "df_transformed['day1'] = df_transformed['day']\n",
    "df_transformed['hour1'] = df_transformed['hour']\n",
    "df_transformed[\"target1\"] = df_transformed[\"target\"]\n",
    "\n",
    "# Define the date ranges for training and testing\n",
    "train_start_date = '2009-07-14'\n",
    "train_end_date = '2010-12-15'\n",
    "test_start_date = '2010-12-15'\n",
    "test_end_date = '2011-01-01'\n",
    "\n",
    "# Convert the date columns to datetime\n",
    "df_transformed['year'] = df_transformed['year'].astype(int)\n",
    "df_transformed['month'] = df_transformed['month'].astype(int)+1\n",
    "df_transformed['day'] = df_transformed['day'].astype(int)+1\n",
    "df_transformed['hour'] = df_transformed['hour'].astype(int)+1\n",
    "\n",
    "# Create a datetime column\n",
    "df_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\n",
    "df_sample = df_transformed.loc[:12825]\n",
    "\n",
    "# Set the datetime column as the index\n",
    "df_transformed.set_index('date_time', inplace=True)\n",
    "df_sample.set_index('date_time', inplace=True)\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "df_transformed.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n",
    "df_sample.drop(columns=['year', 'month', 'day', 'hour','target'], inplace=True)\n",
    "\n",
    "df_transformed = df_transformed.sort_index()\n",
    "df_sample = df_sample.sort_index()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = df_transformed.loc[train_start_date:train_end_date]\n",
    "\n",
    "test = df_transformed.loc[test_start_date:test_end_date]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the first 40 columns of the training data\n",
    "scaler.fit(train.iloc[:, :40])\n",
    "\n",
    "# Transform the first 40 columns of both training and validation data\n",
    "train.iloc[:, :40] = scaler.transform(train.iloc[:, :40])\n",
    "test.iloc[:, :40] = scaler.transform(test.iloc[:, :40])\n",
    "\n",
    "# Create sample data for the first building\n",
    "sample = df_sample.loc[test_start_date:test_end_date]\n",
    "\n",
    "sample.iloc[:, :40] = scaler.transform(sample.iloc[:, :40])\n",
    "\n",
    "del df_transformed, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cefcad-ea56-43f2-a21c-fc9583941c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2009-07-14 00:00:00  2009-07-14 01:00:00  2009-07-14 02:00:00  \\\n",
      "1                  0.692                0.761                0.725   \n",
      "2                  1.310                2.360                1.693   \n",
      "3                  0.177                0.324                0.317   \n",
      "4                  0.048                0.183                0.101   \n",
      "5                  0.614                1.142                1.139   \n",
      "..                   ...                  ...                  ...   \n",
      "908                0.128                0.297                0.284   \n",
      "909                0.229                0.402                0.348   \n",
      "912                0.626                0.691                0.375   \n",
      "913                0.179                0.183                0.178   \n",
      "915                0.448                0.372                0.292   \n",
      "\n",
      "     2009-07-14 03:00:00  2009-07-14 04:00:00  2009-07-14 05:00:00  \\\n",
      "1                  0.546                0.729                0.755   \n",
      "2                  1.738                0.833                0.789   \n",
      "3                  0.311                0.305                0.304   \n",
      "4                  0.175                1.062                0.322   \n",
      "5                  0.836                0.805                0.826   \n",
      "..                   ...                  ...                  ...   \n",
      "908                0.663                0.250                0.183   \n",
      "909                0.297                0.393                0.339   \n",
      "912                0.491                0.386                0.252   \n",
      "913                0.199                0.183                0.172   \n",
      "915                0.292                0.283                0.287   \n",
      "\n",
      "     2009-07-14 06:00:00  2009-07-14 07:00:00  2009-07-14 08:00:00  \\\n",
      "1                  0.773                0.743                0.911   \n",
      "2                  0.709                0.756                1.397   \n",
      "3                  0.303                0.300                1.213   \n",
      "4                  0.151                0.241                0.157   \n",
      "5                  0.511                0.707                0.447   \n",
      "..                   ...                  ...                  ...   \n",
      "908                0.321                0.184                0.219   \n",
      "909                0.306                0.382                0.564   \n",
      "912                0.508                0.539                0.875   \n",
      "913                0.172                0.181                0.449   \n",
      "915                0.289                3.976                0.354   \n",
      "\n",
      "     2009-07-14 09:00:00  ...  2010-12-31 14:00:00  2010-12-31 15:00:00  \\\n",
      "1                  0.579  ...                0.809                1.901   \n",
      "2                  4.208  ...                0.782                0.961   \n",
      "3                  0.730  ...                1.998                1.540   \n",
      "4                  0.117  ...                0.744                6.229   \n",
      "5                  0.296  ...                3.103                2.959   \n",
      "..                   ...  ...                  ...                  ...   \n",
      "908                0.621  ...                0.125                0.149   \n",
      "909                0.499  ...                0.930                0.755   \n",
      "912                0.673  ...                1.560                1.561   \n",
      "913                0.390  ...                0.579                1.443   \n",
      "915                0.276  ...                5.138                1.230   \n",
      "\n",
      "     2010-12-31 16:00:00  2010-12-31 17:00:00  2010-12-31 18:00:00  \\\n",
      "1                  2.744                4.013                3.007   \n",
      "2                  2.522                5.785                6.388   \n",
      "3                  4.620                5.607                6.813   \n",
      "4                  5.182                6.428                7.117   \n",
      "5                  2.676                5.144                6.581   \n",
      "..                   ...                  ...                  ...   \n",
      "908                0.119                0.194                0.197   \n",
      "909                2.359                1.301                2.171   \n",
      "912                4.066                3.383                1.229   \n",
      "913                0.646                1.480                0.585   \n",
      "915                1.054                1.582                1.952   \n",
      "\n",
      "     2010-12-31 19:00:00  2010-12-31 20:00:00  2010-12-31 21:00:00  \\\n",
      "1                  2.166                1.617                1.636   \n",
      "2                  3.271                2.947                3.106   \n",
      "3                  4.491                1.505                1.457   \n",
      "4                  6.624                3.757                2.395   \n",
      "5                  6.868                2.603                2.259   \n",
      "..                   ...                  ...                  ...   \n",
      "908                0.319                0.293                0.245   \n",
      "909                4.659                5.219                1.733   \n",
      "912                1.201                3.447                1.886   \n",
      "913                0.732                0.589                0.582   \n",
      "915                1.666                1.675                0.877   \n",
      "\n",
      "     2010-12-31 22:00:00  2010-12-31 23:00:00  \n",
      "1                  1.700                1.674  \n",
      "2                  3.241                2.846  \n",
      "3                  1.089                1.124  \n",
      "4                  2.286                2.213  \n",
      "5                  1.688                1.628  \n",
      "..                   ...                  ...  \n",
      "908                0.166                0.120  \n",
      "909                1.957                1.743  \n",
      "912                2.004                1.569  \n",
      "913                0.591                1.479  \n",
      "915                0.876                0.879  \n",
      "\n",
      "[743 rows x 12864 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize only the first 40 columns\n",
    "sample.iloc[:, :40] = scaler.fit_transform(sample.iloc[:, :40])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b3095-b315-4c58-a3fb-e354c77d27b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lag_1     24.517\n",
       "lag_2     24.517\n",
       "lag_3     24.517\n",
       "lag_4     24.517\n",
       "lag_5     24.517\n",
       "lag_6     24.517\n",
       "lag_7     24.517\n",
       "lag_8     24.517\n",
       "lag_9     24.517\n",
       "lag_10    24.517\n",
       "lag_11    24.517\n",
       "lag_12    24.517\n",
       "lag_13    24.517\n",
       "lag_14    24.517\n",
       "lag_15    24.517\n",
       "lag_16    24.517\n",
       "lag_17    24.517\n",
       "lag_18    24.517\n",
       "lag_19    24.517\n",
       "lag_20    24.517\n",
       "lag_21    24.517\n",
       "lag_22    24.517\n",
       "lag_23    24.517\n",
       "lag_24    24.517\n",
       "lag_25    24.517\n",
       "lag_26    24.517\n",
       "lag_27    24.517\n",
       "lag_28    24.517\n",
       "lag_29    24.517\n",
       "lag_30    24.517\n",
       "lag_31    24.517\n",
       "lag_32    24.517\n",
       "lag_33    24.517\n",
       "lag_34    24.517\n",
       "lag_35    24.517\n",
       "lag_36    24.517\n",
       "lag_37    24.517\n",
       "lag_38    24.517\n",
       "lag_39    24.517\n",
       "lag_40    24.517\n",
       "dtype: float32"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.iloc[:, :40].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fe866-a752-4d4c-a006-d14552e3f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_8</th>\n",
       "      <th>lag_9</th>\n",
       "      <th>lag_10</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_36</th>\n",
       "      <th>lag_37</th>\n",
       "      <th>lag_38</th>\n",
       "      <th>lag_39</th>\n",
       "      <th>lag_40</th>\n",
       "      <th>year1</th>\n",
       "      <th>month1</th>\n",
       "      <th>day1</th>\n",
       "      <th>hour1</th>\n",
       "      <th>target1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-12-15 00:00:00</th>\n",
       "      <td>0.057708</td>\n",
       "      <td>0.070939</td>\n",
       "      <td>0.079591</td>\n",
       "      <td>0.056690</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.119895</td>\n",
       "      <td>0.046106</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>0.043052</td>\n",
       "      <td>0.093331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350626</td>\n",
       "      <td>0.184117</td>\n",
       "      <td>0.315105</td>\n",
       "      <td>0.208035</td>\n",
       "      <td>0.301671</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-15 01:00:00</th>\n",
       "      <td>0.067060</td>\n",
       "      <td>0.075238</td>\n",
       "      <td>0.053590</td>\n",
       "      <td>0.181650</td>\n",
       "      <td>0.113339</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.037427</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.088227</td>\n",
       "      <td>0.244476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174049</td>\n",
       "      <td>0.297874</td>\n",
       "      <td>0.196659</td>\n",
       "      <td>0.285174</td>\n",
       "      <td>0.330683</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-15 02:00:00</th>\n",
       "      <td>0.074206</td>\n",
       "      <td>0.052855</td>\n",
       "      <td>0.179158</td>\n",
       "      <td>0.111784</td>\n",
       "      <td>0.042986</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>0.040140</td>\n",
       "      <td>0.087017</td>\n",
       "      <td>0.241123</td>\n",
       "      <td>0.234385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293788</td>\n",
       "      <td>0.193961</td>\n",
       "      <td>0.281262</td>\n",
       "      <td>0.326147</td>\n",
       "      <td>0.177829</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-15 03:00:00</th>\n",
       "      <td>0.052214</td>\n",
       "      <td>0.176984</td>\n",
       "      <td>0.110427</td>\n",
       "      <td>0.042465</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.039653</td>\n",
       "      <td>0.085961</td>\n",
       "      <td>0.238197</td>\n",
       "      <td>0.231541</td>\n",
       "      <td>0.327908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191608</td>\n",
       "      <td>0.277850</td>\n",
       "      <td>0.322189</td>\n",
       "      <td>0.175671</td>\n",
       "      <td>0.171734</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-15 04:00:00</th>\n",
       "      <td>0.176760</td>\n",
       "      <td>0.110288</td>\n",
       "      <td>0.042411</td>\n",
       "      <td>0.036419</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>0.085852</td>\n",
       "      <td>0.237895</td>\n",
       "      <td>0.231248</td>\n",
       "      <td>0.327492</td>\n",
       "      <td>0.157567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277498</td>\n",
       "      <td>0.321781</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>0.171517</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-31 20:00:00</th>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.046326</td>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.042806</td>\n",
       "      <td>0.088559</td>\n",
       "      <td>0.088886</td>\n",
       "      <td>0.126127</td>\n",
       "      <td>0.183747</td>\n",
       "      <td>0.178836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.155592</td>\n",
       "      <td>0.224589</td>\n",
       "      <td>0.328453</td>\n",
       "      <td>0.246115</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-31 21:00:00</th>\n",
       "      <td>0.045706</td>\n",
       "      <td>0.036258</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>0.042234</td>\n",
       "      <td>0.087374</td>\n",
       "      <td>0.087697</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>0.181289</td>\n",
       "      <td>0.176444</td>\n",
       "      <td>0.216901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153510</td>\n",
       "      <td>0.221585</td>\n",
       "      <td>0.324060</td>\n",
       "      <td>0.242823</td>\n",
       "      <td>0.174910</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-31 22:00:00</th>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.037112</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.086728</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>0.123519</td>\n",
       "      <td>0.179948</td>\n",
       "      <td>0.175139</td>\n",
       "      <td>0.215297</td>\n",
       "      <td>0.088972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219946</td>\n",
       "      <td>0.321662</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.173616</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-31 23:00:00</th>\n",
       "      <td>0.036820</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>0.086046</td>\n",
       "      <td>0.086364</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.178534</td>\n",
       "      <td>0.173763</td>\n",
       "      <td>0.213605</td>\n",
       "      <td>0.088273</td>\n",
       "      <td>0.111256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319135</td>\n",
       "      <td>0.239133</td>\n",
       "      <td>0.172252</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>0.130103</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>0.041244</td>\n",
       "      <td>0.085327</td>\n",
       "      <td>0.085643</td>\n",
       "      <td>0.121525</td>\n",
       "      <td>0.177043</td>\n",
       "      <td>0.172311</td>\n",
       "      <td>0.211820</td>\n",
       "      <td>0.087536</td>\n",
       "      <td>0.110326</td>\n",
       "      <td>0.104806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237135</td>\n",
       "      <td>0.170813</td>\n",
       "      <td>0.127518</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>0.134064</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lag_1     lag_2     lag_3     lag_4     lag_5  \\\n",
       "date_time                                                               \n",
       "2010-12-15 00:00:00  0.057708  0.070939  0.079591  0.056690  0.192157   \n",
       "2010-12-15 01:00:00  0.067060  0.075238  0.053590  0.181650  0.113339   \n",
       "2010-12-15 02:00:00  0.074206  0.052855  0.179158  0.111784  0.042986   \n",
       "2010-12-15 03:00:00  0.052214  0.176984  0.110427  0.042465  0.036465   \n",
       "2010-12-15 04:00:00  0.176760  0.110288  0.042411  0.036419  0.039602   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2010-12-31 20:00:00  0.064250  0.046326  0.036749  0.037895  0.042806   \n",
       "2010-12-31 21:00:00  0.045706  0.036258  0.037388  0.042234  0.087374   \n",
       "2010-12-31 22:00:00  0.035990  0.037112  0.041921  0.086728  0.087048   \n",
       "2010-12-31 23:00:00  0.036820  0.041592  0.086046  0.086364  0.122549   \n",
       "2011-01-01 00:00:00  0.041244  0.085327  0.085643  0.121525  0.177043   \n",
       "\n",
       "                        lag_6     lag_7     lag_8     lag_9    lag_10  ...  \\\n",
       "date_time                                                              ...   \n",
       "2010-12-15 00:00:00  0.119895  0.046106  0.039592  0.043052  0.093331  ...   \n",
       "2010-12-15 01:00:00  0.043584  0.037427  0.040698  0.088227  0.244476  ...   \n",
       "2010-12-15 02:00:00  0.036913  0.040140  0.087017  0.241123  0.234385  ...   \n",
       "2010-12-15 03:00:00  0.039653  0.085961  0.238197  0.231541  0.327908  ...   \n",
       "2010-12-15 04:00:00  0.085852  0.237895  0.231248  0.327492  0.157567  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "2010-12-31 20:00:00  0.088559  0.088886  0.126127  0.183747  0.178836  ...   \n",
       "2010-12-31 21:00:00  0.087697  0.124440  0.181289  0.176444  0.216901  ...   \n",
       "2010-12-31 22:00:00  0.123519  0.179948  0.175139  0.215297  0.088972  ...   \n",
       "2010-12-31 23:00:00  0.178534  0.173763  0.213605  0.088273  0.111256  ...   \n",
       "2011-01-01 00:00:00  0.172311  0.211820  0.087536  0.110326  0.104806  ...   \n",
       "\n",
       "                       lag_36    lag_37    lag_38    lag_39    lag_40  year1  \\\n",
       "date_time                                                                      \n",
       "2010-12-15 00:00:00  0.350626  0.184117  0.315105  0.208035  0.301671   2010   \n",
       "2010-12-15 01:00:00  0.174049  0.297874  0.196659  0.285174  0.330683   2010   \n",
       "2010-12-15 02:00:00  0.293788  0.193961  0.281262  0.326147  0.177829   2010   \n",
       "2010-12-15 03:00:00  0.191608  0.277850  0.322189  0.175671  0.171734   2010   \n",
       "2010-12-15 04:00:00  0.277498  0.321781  0.175449  0.171517  0.072464   2010   \n",
       "...                       ...       ...       ...       ...       ...    ...   \n",
       "2010-12-31 20:00:00  0.066215  0.155592  0.224589  0.328453  0.246115   2010   \n",
       "2010-12-31 21:00:00  0.153510  0.221585  0.324060  0.242823  0.174910   2010   \n",
       "2010-12-31 22:00:00  0.219946  0.321662  0.241026  0.173616  0.129611   2010   \n",
       "2010-12-31 23:00:00  0.319135  0.239133  0.172252  0.128592  0.130103   2010   \n",
       "2011-01-01 00:00:00  0.237135  0.170813  0.127518  0.129016  0.134064   2010   \n",
       "\n",
       "                     month1  day1  hour1  target1  \n",
       "date_time                                          \n",
       "2010-12-15 00:00:00    11.0  13.0   23.0    3.437  \n",
       "2010-12-15 01:00:00    11.0  14.0    0.0    1.874  \n",
       "2010-12-15 02:00:00    11.0  14.0    1.0    1.832  \n",
       "2010-12-15 03:00:00    11.0  14.0    2.0    0.774  \n",
       "2010-12-15 04:00:00    11.0  14.0    3.0    0.723  \n",
       "...                     ...   ...    ...      ...  \n",
       "2010-12-31 20:00:00    11.0  30.0   19.0    2.166  \n",
       "2010-12-31 21:00:00    11.0  30.0   20.0    1.617  \n",
       "2010-12-31 22:00:00    11.0  30.0   21.0    1.636  \n",
       "2010-12-31 23:00:00    11.0  30.0   22.0    1.700  \n",
       "2011-01-01 00:00:00    11.0  30.0   23.0    1.674  \n",
       "\n",
       "[409 rows x 45 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddaa8ae-d4d9-4d9f-b5ec-e632e23b51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx].astype(\"float32\")\n",
    "        features = torch.FloatTensor(row[:-5].values)  # All columns except the last 5\n",
    "        year = torch.FloatTensor([row[-5]]).to(torch.int)\n",
    "        month = torch.FloatTensor([row[-4]]).to(torch.int)  # Month is expected to be 0-11\n",
    "        day = torch.FloatTensor([row[-3]]).to(torch.int)    # Day is expected to be 0-30\n",
    "        hour = torch.FloatTensor([row[-2]]).to(torch.int)      # Hour is already 0-indexed (0-23)\n",
    "        label = torch.FloatTensor([row[-1]])  # The last column\n",
    "        return features, year, month, day, hour, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "def create_data_loaders(data, chunk_size, batch_size):\n",
    "    chunks = np.array_split(data, len(data) // chunk_size)\n",
    "    data_loaders = []\n",
    "    for chunk in chunks:\n",
    "        dataset = CustomDataset(chunk)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        data_loaders.append(data_loader)\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def train_function(net, criterion, optimizer, data_loaders, n_epochs=5, device=torch.device(\"cuda\")):\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, threshold=0.1, patience=3, factor=0.5)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_loss = 0\n",
    "        counter = 0\n",
    "        for data_loader in data_loaders:\n",
    "            for features, year, month, day, hour, labels in tqdm(data_loader):\n",
    "                features = features.to(device)\n",
    "                year = year.to(device)\n",
    "                month = month.to(device)\n",
    "                day = day.to(device)\n",
    "                hour = hour.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(features, month, day, hour, year)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(f\"epoch {epoch}\")\n",
    "        scheduler.step(epoch_loss / len(data_loaders))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loaders)}\")\n",
    "        with open(f'modelcheckpoint{epoch}.pickle', 'wb') as handle:\n",
    "            pickle.dump([net, optimizer], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        collected = gc.collect()\n",
    "    return net\n",
    "\n",
    "def test_function(net, data_loaders, scaler, label_scaler, device=torch.device(\"cuda\"), return_data=False):\n",
    "    mse = MeanSquaredError().to(device)\n",
    "    smape = SymmetricMeanAbsolutePercentageError().to(device)\n",
    "    net.eval()\n",
    "    list_outputs = []\n",
    "    list_targets = []\n",
    "    with torch.no_grad():  # to not reserve a memory space for gradients\n",
    "        for data_loader in data_loaders:\n",
    "            for features, year, month, day, hour, labels in tqdm(data_loader):\n",
    "                features = features.to(device)\n",
    "                year = year.to(device)\n",
    "                month = month.to(device)\n",
    "                day = day.to(device)\n",
    "                hour = hour.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(features, month, day, hour, year)\n",
    "                outputs = outputs.squeeze()\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                mse(outputs, labels)\n",
    "                smape(outputs, labels)\n",
    "                list_targets.append(labels.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n",
    "                list_outputs.append(outputs.detach().unsqueeze(0))  # Ensure tensors are at least one-dimensional\n",
    "    test_mse = mse.compute()\n",
    "    test_smape = smape.compute()\n",
    "    print(f\"Test MSE: {test_mse} , SMAPE {test_smape}\")\n",
    "    if return_data:\n",
    "        return torch.cat(list_outputs), torch.cat(list_targets), test_mse, test_smape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9b802-3c4a-4d4d-a8e4-bdf1b7a64625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to create the model for memory profiling\n",
    "def create_model():\n",
    "    model = CombinedModel(nbits_params, embedding_dim, final_hidden)\n",
    "    return model\n",
    "\n",
    "def get_memory_usage():\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "class TrendBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers):\n",
    "        super(TrendBlock, self).__init__()\n",
    "        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n",
    "                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n",
    "        self.backcast_fc = nn.Linear(num_hidden, input_size)\n",
    "        self.forecast_fc = nn.Linear(num_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = torch.relu(layer(x))\n",
    "        backcast = self.backcast_fc(x)\n",
    "        forecast = self.forecast_fc(x)\n",
    "        return backcast, forecast\n",
    "\n",
    "class SeasonalityBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers):\n",
    "        super(SeasonalityBlock, self).__init__()\n",
    "        self.fc = nn.ModuleList([nn.Linear(input_size, num_hidden)] +\n",
    "                                [nn.Linear(num_hidden, num_hidden) for _ in range(num_layers - 1)])\n",
    "        self.backcast_fc = nn.Linear(num_hidden, input_size)\n",
    "        self.forecast_fc = nn.Linear(num_hidden, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc:\n",
    "            x = torch.relu(layer(x))\n",
    "        backcast = self.backcast_fc(x)\n",
    "        forecast = self.forecast_fc(x)\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBEATS(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden, num_layers, num_blocks):\n",
    "        super(NBEATS, self).__init__()\n",
    "        self.trend_blocks = nn.ModuleList([TrendBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n",
    "        self.seasonality_blocks = nn.ModuleList([SeasonalityBlock(input_size, output_size, num_hidden, num_layers) for _ in range(num_blocks // 2)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        backcast, forecast = x, 0\n",
    "        for block in self.trend_blocks:\n",
    "            backcast, block_forecast = block(x)\n",
    "            x = x - backcast\n",
    "            forecast = forecast + block_forecast\n",
    "        for block in self.seasonality_blocks:\n",
    "            backcast, block_forecast = block(x)\n",
    "            x = x - backcast\n",
    "            forecast = forecast + block_forecast\n",
    "        return forecast\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class YearNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(YearNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, 25)\n",
    "        self.output = nn.Linear(25, num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        return self.output(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nbits_params, embedding_dim, final_hidden):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.nbeats = NBEATS(**nbits_params)\n",
    "        self.month_net = EmbeddingNetwork(12, embedding_dim)  # Months from 1 to 12\n",
    "        self.day_net = EmbeddingNetwork(31, embedding_dim)    # Days from 1 to 31\n",
    "        self.hour_net = EmbeddingNetwork(24, embedding_dim)   # Hours from 0 to 23\n",
    "        self.year_net = YearNetwork(1, 10)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            #nn.Linear(embedding_dim * 3 + nbits_params['output_size'] + 10, final_hidden),\n",
    "            nn.Linear(embedding_dim * 3 + nbits_params['output_size'], final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, month, day, hour, year):\n",
    "        ts_output = self.nbeats(x)\n",
    "        #print('TS', ts_output)\n",
    "        month_output = self.month_net(month.long()).squeeze(1)\n",
    "        #print('Month', month_output)\n",
    "        day_output = self.day_net(day.long()).squeeze(1)\n",
    "        #print('Day', day_output)\n",
    "        hour_output = self.hour_net(hour.long()).squeeze(1)\n",
    "        #print('Hour', hour_output)\n",
    "        \n",
    "        #print('Year', year_output)\n",
    "        #combined_output = torch.cat((ts_output, month_output, day_output, hour_output, year_output), dim=1)\n",
    "        combined_output = torch.cat((ts_output, month_output, day_output, hour_output), dim=1)\n",
    "        final_output = self.final_layer(combined_output)\n",
    "        return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915da287-825c-49ac-ac5a-1e38bfd2dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 40  # Length of input time series\n",
    "output_size = 1  # Length of output time series (forecast)\n",
    "num_blocks = 12\n",
    "num_hidden = 512\n",
    "num_layers = 8\n",
    "embedding_dim = 10\n",
    "final_hidden = 256\n",
    "\n",
    "nbits_params = {\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'num_blocks': num_blocks,\n",
    "    'num_hidden': num_hidden,\n",
    "    'num_layers': num_layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065ba64-f246-4eb3-8dac-bd3cde7389ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf404f-b16f-4ffb-85aa-693ad1599320",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load('trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54946003-24ca-4314-8094-917f235075e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (nbeats): NBEATS(\n",
       "    (trend_blocks): ModuleList(\n",
       "      (0-5): 6 x TrendBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=512, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (seasonality_blocks): ModuleList(\n",
       "      (0-5): 6 x SeasonalityBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=512, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (month_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(12, 10)\n",
       "  )\n",
       "  (day_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(31, 10)\n",
       "  )\n",
       "  (hour_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(24, 10)\n",
       "  )\n",
       "  (year_net): YearNetwork(\n",
       "    (hidden): Linear(in_features=1, out_features=25, bias=True)\n",
       "    (output): Linear(in_features=25, out_features=10, bias=True)\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea185d1-6738-4327-b5d8-bb152732963e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (nbeats): NBEATS(\n",
       "    (trend_blocks): ModuleList(\n",
       "      (0-5): 6 x TrendBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=512, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (seasonality_blocks): ModuleList(\n",
       "      (0-5): 6 x SeasonalityBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=512, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=512, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (month_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(12, 10)\n",
       "  )\n",
       "  (day_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(31, 10)\n",
       "  )\n",
       "  (hour_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(24, 10)\n",
       "  )\n",
       "  (year_net): YearNetwork(\n",
       "    (hidden): Linear(in_features=1, out_features=25, bias=True)\n",
       "    (output): Linear(in_features=25, out_features=10, bias=True)\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr = 0.0005\n",
    "n_epochs = 10\n",
    "window_size = 40\n",
    "chunk_size = 122880\n",
    "batch_size = 1024*12*5\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# Assuming `train`, `test`, `sample` are pre-loaded DataFrames\n",
    "train_loaders = create_data_loaders(train, chunk_size, batch_size)\n",
    "test_loaders = create_data_loaders(test, chunk_size, batch_size)\n",
    "sample_loaders = create_data_loaders(sample, 100, 1)\n",
    "\n",
    "device = \"cuda\"\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df2e2a-f104-45ed-8363-603b8c432b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde57410b5d54bb18b48c683e36a4ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af0dc3e81cb41478e153f26ae446051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.037216067314148 , SMAPE 0.4622192978858948\n"
     ]
    }
   ],
   "source": [
    "*_, test_mse, test_smape = test_function(trained_model, test_loaders, None, None, torch.device(device), return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d5587-b1cb-4815-afab-e533857cfee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0372, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08171d-a10e-49b1-b849-9af38d08948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4622, device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027694ee-7a4a-410a-a66c-4c4672c0b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp\n",
    "from torch_pruning.pruner import function\n",
    "from fasterai.core.all import *\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import quantization\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from fasterbench.benchmark import *\n",
    "\n",
    "def get_ignored_layers(model):\n",
    "    ignored_layers = []\n",
    "\n",
    "    # Check and process trend blocks if they exist\n",
    "    if hasattr(model.nbeats, 'trend_blocks'):\n",
    "        for block in model.nbeats.trend_blocks:\n",
    "            if hasattr(block, 'backcast_fc'):\n",
    "                ignored_layers.append(block.backcast_fc)\n",
    "            if hasattr(block, 'forecast_fc'):\n",
    "                ignored_layers.append(block.forecast_fc)\n",
    "\n",
    "    # Check and process seasonality blocks if they exist\n",
    "    if hasattr(model.nbeats, 'seasonality_blocks'):\n",
    "        for block in model.nbeats.seasonality_blocks:\n",
    "            if hasattr(block, 'backcast_fc'):\n",
    "                ignored_layers.append(block.backcast_fc)\n",
    "            if hasattr(block, 'forecast_fc'):\n",
    "                ignored_layers.append(block.forecast_fc)\n",
    "\n",
    "    return ignored_layers\n",
    "\n",
    "def adjust_layer_features(layer, pruning_ratio):\n",
    "    if hasattr(layer, 'in_features') and hasattr(layer, 'weight'):\n",
    "        in_features = layer.in_features\n",
    "        layer.in_features = int(in_features * (1-pruning_ratio))\n",
    "        local_scores = large_final(layer, 'column')\n",
    "        threshold = torch.quantile(local_scores.view(-1), pruning_ratio)\n",
    "        mask = local_scores.ge(threshold).to(dtype=local_scores.dtype)\n",
    "        ixs = torch.nonzero(mask[0] == 1, as_tuple=True)[0]\n",
    "        layer.weight.data = layer.weight[:, ixs]\n",
    "\n",
    "def prune_model(model, pruning_ratio, dummy_input):\n",
    "    imp = tp.importance.GroupNormImportance(p=2)\n",
    "    ignored_layers = get_ignored_layers(model)\n",
    "\n",
    "    pruner = tp.pruner.MetaPruner(\n",
    "        model.nbeats,\n",
    "        dummy_input,\n",
    "        importance=imp,\n",
    "        pruning_ratio=pruning_ratio, \n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "    pruner.step()\n",
    "    \n",
    "    if hasattr(model.nbeats, 'trend_blocks'):\n",
    "        for block in model.nbeats.trend_blocks:\n",
    "            for layer in [block.backcast_fc, block.forecast_fc]:\n",
    "                if layer is not None and layer.in_features != int(num_hidden*(1-pruning_ratio)):\n",
    "                    adjust_layer_features(layer, pruning_ratio)\n",
    "\n",
    "    if hasattr(model.nbeats, 'seasonality_blocks'):\n",
    "        for block in model.nbeats.seasonality_blocks:\n",
    "            for layer in [block.backcast_fc, block.forecast_fc]:\n",
    "                if layer is not None and layer.in_features != int(num_hidden*(1-pruning_ratio)):\n",
    "                    adjust_layer_features(layer, pruning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144715e-4391-47fb-959a-f7926a5ecba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_parameters \u001b[38;5;241m=\u001b[39m get_num_parameters(\u001b[43mtrained_model\u001b[49m)\n\u001b[1;32m      2\u001b[0m disk_size \u001b[38;5;241m=\u001b[39m get_model_size(trained_model)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisk_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB (disk), \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(trained_model)\n",
    "disk_size = get_model_size(trained_model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9cd4ef-a734-4535-b527-63f1cad7c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_features = 40\n",
    "\n",
    "features = torch.randn(batch_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458ae8b-3b4e-48ea-a48a-e0ecb040a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load('trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35168432-3705-4c78-9b20-74d8ce1629cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(trained_model, 0.15, features.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ac995-38ce-454b-a83f-3c0117022b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84a4a7-3f5a-433d-8280-0a58a578b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 65.56 MB (disk), 16369401 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(trained_model)\n",
    "disk_size = get_model_size(trained_model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace20f89-1479-4177-89f0-692507888ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c834263153da4f8e95b83c475ac95754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e71e5cd5dd4dae908db0d5a7ad2d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.1977158784866333 , SMAPE 0.4805765748023987\n"
     ]
    }
   ],
   "source": [
    "*_, test_mse, test_smape = test_function(trained_model, test_loaders, None, None, torch.device(device), return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572fe6e8-8fef-459e-bf8a-49ca29df6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.9425, device='cuda:0'), tensor(0.7663, device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse, test_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68909950-270a-419d-b87b-b7f6aee88670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42751d-9856-4553-815c-81c8583fbeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def script_model(model, dummy_input, path='scripted_model.pt'):\n",
    "    scripted_model = torch.jit.trace(model, dummy_input)\n",
    "    scripted_model.save(path)\n",
    "    return scripted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210369-795b-4e58-b7bc-df860ba99d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_onnx(model, dummy_input, onnx_path=\"model.onnx\", quant_onnx_path=\"model_quantized.onnx\"):\n",
    "    torch.onnx.export(\n",
    "    model,              \n",
    "    dummy_input,        \n",
    "    onnx_path, \n",
    "    input_names=[\"features\", \"year\", \"month\", \"day\", \"hour\"],   \n",
    "    output_names=[\"output\"], \n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}, \n",
    "    opset_version=11\n",
    "    )\n",
    "\n",
    "    quantize_dynamic(\n",
    "        onnx_path,\n",
    "        quant_onnx_path,\n",
    "        weight_type=QuantType.QUInt8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbf777-784e-43e2-a11e-908a50c48ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_features = 40\n",
    "\n",
    "features = torch.randn(batch_size, num_features)\n",
    "month = torch.randint(0, 12, (batch_size, 1))      # Random months between 1 and 12\n",
    "day = torch.randint(0, 31, (batch_size, 1))        # Random days between 1 and 31\n",
    "hour = torch.randint(0, 24, (batch_size, 1))       # Random hours between 0 and 23\n",
    "year = torch.randint(2019, 2021, (batch_size, 1)) \n",
    "\n",
    "example_input = features, year, month, day, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d42009-f96c-41b6-b212-a2d94dea2f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (nbeats): NBEATS(\n",
       "    (trend_blocks): ModuleList(\n",
       "      (0-5): 6 x TrendBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=435, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (seasonality_blocks): ModuleList(\n",
       "      (0-5): 6 x SeasonalityBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=435, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (month_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(12, 10)\n",
       "  )\n",
       "  (day_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(31, 10)\n",
       "  )\n",
       "  (hour_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(24, 10)\n",
       "  )\n",
       "  (year_net): YearNetwork(\n",
       "    (hidden): Linear(in_features=1, out_features=25, bias=True)\n",
       "    (output): Linear(in_features=25, out_features=10, bias=True)\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7292e-37af-4911-9df6-b4cabc9433a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, x, month, day, hour, year)\u001b[0m\n\u001b[1;32m     91\u001b[0m ts_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbeats(x)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#print('TS', ts_output)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m month_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#print('Month', month_output)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m day_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_net(day\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mEmbeddingNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "trained_model.to('cpu')(*example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048f3b2-7f0d-4cb2-93a6-21abb4ba4834",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scripted_model \u001b[38;5;241m=\u001b[39m \u001b[43mscript_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m quantize_onnx(net, example_input)\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mscript_model\u001b[0;34m(model, dummy_input, path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscript_model\u001b[39m(model, dummy_input, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscripted_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     scripted_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     scripted_model\u001b[38;5;241m.\u001b[39msave(path)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scripted_model\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/jit/_trace.py:806\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m ):\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/jit/_trace.py:1074\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1073\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1074\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, x, month, day, hour, year)\u001b[0m\n\u001b[1;32m     91\u001b[0m ts_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbeats(x)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#print('TS', ts_output)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m month_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#print('Month', month_output)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m day_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_net(day\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mEmbeddingNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "scripted_model = script_model(trained_model.to('cpu'), example_input)\n",
    "quantize_onnx(net, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d68ee-7cd1-43a2-a506-cb2fe1fbf8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (nbeats): NBEATS(\n",
       "    (trend_blocks): ModuleList(\n",
       "      (0-5): 6 x TrendBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=435, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (seasonality_blocks): ModuleList(\n",
       "      (0-5): 6 x SeasonalityBlock(\n",
       "        (fc): ModuleList(\n",
       "          (0): Linear(in_features=40, out_features=435, bias=True)\n",
       "          (1-7): 7 x Linear(in_features=435, out_features=435, bias=True)\n",
       "        )\n",
       "        (backcast_fc): Linear(in_features=435, out_features=40, bias=True)\n",
       "        (forecast_fc): Linear(in_features=435, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (month_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(12, 10)\n",
       "  )\n",
       "  (day_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(31, 10)\n",
       "  )\n",
       "  (hour_net): EmbeddingNetwork(\n",
       "    (embedding): Embedding(24, 10)\n",
       "  )\n",
       "  (year_net): YearNetwork(\n",
       "    (hidden): Linear(in_features=1, out_features=25, bias=True)\n",
       "    (output): Linear(in_features=25, out_features=10, bias=True)\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7a820-ed26-465f-9f1e-d04a99ae8ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
