{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Create a preprocess module\n",
    "output-file: data.preprocess.html\n",
    "title: Preprocess\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.preprocess\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "import gc\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weekday_average(data:dict # pandas dictionnary containing the data\n",
    "                   )->list[float]:\n",
    "    \"compute the weekday average\"\n",
    "    weekday_avgs = data.groupby(data.index.weekday).mean() * 24\n",
    "    return weekday_avgs.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def day_segment_average(data:dict # pandas dictionnary containing the data\n",
    "                       )->list[float]:\n",
    "    \"compute the daily average\"\n",
    "    segments = {\n",
    "        'early_morning': (7, 9),\n",
    "        'morning': (9, 13),\n",
    "        'early_afternoon': (13, 17),\n",
    "        'late_afternoon': (17, 21),\n",
    "        'night': [(21, 24), (0, 7)]\n",
    "    }\n",
    "    averages = []\n",
    "    for segment, hours in segments.items():\n",
    "        if segment == 'night':\n",
    "            energy = data.between_time('21:00', '23:59').mean() * 10 + data.between_time('00:00', '06:59').mean() * 10\n",
    "        elif  segment == 'early_morning':\n",
    "            start, end = hours\n",
    "            energy = data.between_time(f'{start}:00', f'{end-1}:59').mean() * 2\n",
    "        else :\n",
    "            start, end = hours\n",
    "            energy = data.between_time(f'{start}:00', f'{end-1}:59').mean() * 4\n",
    "            \n",
    "        averages.append(energy)\n",
    "    return np.array(averages).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def total_energy_used(data:dict # pandas dictionnary containing the data\n",
    "                     )->float:\n",
    "    return data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def average_energy_used(data:dict # pandas dictionnary containing the data\n",
    "                       )->list[float]:\n",
    "    \"compute the average energy used\"\n",
    "    hourly_avg = data.mean()\n",
    "    daily_avg = data.resample('D').sum().mean()\n",
    "    weekly_avg = data.resample('W').sum().mean()\n",
    "    monthly_avg = data.resample('M').sum().mean()\n",
    "    return np.array([hourly_avg, daily_avg, weekly_avg, monthly_avg]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weekend_businessday_avg(data:dict # pandas dictionnary containing the data\n",
    "                           )->list[float]:\n",
    "    \"compute the weekend average\"\n",
    "    weekends_avg = data[data.index.weekday >= 5].resample('D').sum().mean() \n",
    "    business_days_avg = data[data.index.weekday < 5].resample('D').sum().mean()\n",
    "    return np.array([weekends_avg, business_days_avg]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resample_building_data(group:dict # pandas dictionnary containing the building data\n",
    "                          )->dict:\n",
    "    \"resample the building data\"\n",
    "    group = group.reset_index(level='ID')\n",
    "    # Specify columns explicitly for summing\n",
    "    resampled_group = group.resample('h').agg({'consumption': 'sum'})  # Example if 'consumption' is your numeric column\n",
    "    resampled_group['ID'] = group['ID'].iloc[0]  # Handle non-numeric separately if needed\n",
    "    resampled_group = resampled_group.set_index('ID', append=True)\n",
    "    return resampled_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def optimize_data_types(df):\n",
    "\n",
    "    for col in df.select_dtypes(include=['int']).columns:\n",
    "        df[col] = df[col].astype('int32')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique_values = len(df[col].unique())\n",
    "        num_total_values = len(df[col])\n",
    "        if num_unique_values / num_total_values < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_lagged_features(df, window_size=40, start_date='2009-07-14 00:00:00'):\n",
    "\n",
    "    data = []\n",
    "    columns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\n",
    "    base_date = pd.Timestamp(start_date)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        values = row.values\n",
    "        for t in range(window_size, len(values)):\n",
    "            lagged_inputs = values[t-window_size:t]\n",
    "            target_value = values[t]\n",
    "\n",
    "            # Calculate the corresponding date and time\n",
    "            timestamp = base_date + pd.Timedelta(hours=t)\n",
    "            year, month, day, hour = timestamp.year, timestamp.month, timestamp.day, timestamp.hour\n",
    "\n",
    "            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n",
    "\n",
    "    df_transformed = pd.DataFrame(data, columns=columns)\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_lagged_features(df, window_size=40, start_date='2009-07-14 00:00:00'):\n",
    "\n",
    "    data = []\n",
    "    columns = [f'lag_{i}' for i in range(1, window_size+1)] + ['year', 'month', 'day', 'hour', 'target']\n",
    "    base_date = pd.Timestamp(start_date)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        values = row.values\n",
    "        for t in range(window_size, len(values)):\n",
    "            lagged_inputs = values[t-window_size:t]\n",
    "            target_value = values[t]\n",
    "\n",
    "            # Calculate the corresponding date and time\n",
    "            timestamp = base_date + pd.Timedelta(hours=t)\n",
    "            year, month, day, hour = timestamp.year, timestamp.month, timestamp.day, timestamp.hour\n",
    "\n",
    "            data.append(list(lagged_inputs) + [year, month, day, hour, target_value])\n",
    "\n",
    "    df_transformed = pd.DataFrame(data, columns=columns)\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preprocess_dataframe(df, window_size=40, chunk_size=100):\n",
    "\n",
    "    # Optimize data types\n",
    "    df = optimize_data_types(df)\n",
    "    \n",
    "    # Remove the first and last columns\n",
    "    df = df.iloc[:, 1:-1]\n",
    "\n",
    "    # Process the data in chunks to create lagged features\n",
    "    processed_data = []\n",
    "    for start_row in tqdm(range(0, df.shape[0], chunk_size)):\n",
    "        chunk = df.iloc[start_row:start_row + chunk_size]\n",
    "        df_chunk_transformed = create_lagged_features(chunk, window_size=window_size)\n",
    "        processed_data.append(df_chunk_transformed)\n",
    "\n",
    "    df_transformed = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Make sure year, month, day, and hour columns are integers\n",
    "    for col in ['year', 'month', 'day', 'hour']:\n",
    "        df_transformed[col] = df_transformed[col].astype(int)\n",
    "\n",
    "    # Create datetime column\n",
    "    df_transformed['date_time'] = pd.to_datetime(df_transformed[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "    # Apply OrdinalEncoder to the month, day, and hour columns\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    df_transformed[['month', 'day', 'hour']] = ordinal_encoder.fit_transform(df_transformed[['month', 'day', 'hour']])\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scale_features(train, test, columns_to_scale):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler on training data and apply the same transformation to both train and test data\n",
    "    train[columns_to_scale] = scaler.fit_transform(train[columns_to_scale])\n",
    "    test[columns_to_scale] = scaler.transform(test[columns_to_scale])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def finalize_dataframe(df_transformed, train_start_date, train_end_date, test_start_date, test_end_date):\n",
    "\n",
    "    # Create the datetime index and sort\n",
    "    df_transformed.set_index('date_time', inplace=True)\n",
    "    df_transformed = df_transformed.sort_index()\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    train = df_transformed.loc[train_start_date:train_end_date]\n",
    "    test = df_transformed.loc[test_start_date:test_end_date]\n",
    "\n",
    "    # Drop unnecessary columns like year, month, day, hour, and target\n",
    "    #train = train.drop(columns=['year', 'month', 'day', 'hour', 'target'])\n",
    "    #test = test.drop(columns=['year', 'month', 'day', 'hour', 'target'])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
